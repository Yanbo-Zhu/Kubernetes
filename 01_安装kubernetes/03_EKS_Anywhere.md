
https://rickhw.github.io/2021/09/23/AWS/Experience-EKS-Anywhere/

AWS æŠŠ EKS æœ¬èº«é–‹æºäº†ï¼Œç¨±ç‚º `EKS Anywhere`ï¼Œé™¤äº†åœ¨ AWS è‡ªå·±ä¹‹å¤–ï¼Œä¹Ÿå¯ä»¥å®‰è£åœ¨ç§æœ‰ (on-premises) çš„ç’°å¢ƒã€‚

æœ¬æ–‡æ•´ç†é«”é©—çš„ç­†è¨˜ï¼Œæˆ‘åœ¨ macOSã€ubuntu20.04 éƒ½æœ‰åšéï¼Œæœ¬æ–‡è¨˜éŒ„å‰‡ä»¥ ubuntu 20.04 ç‚ºä¸»ï¼Œå…§å®¹ä»¥åƒè€ƒ [EKS Anywhere](https://anywhere.eks.amazonaws.com/docs/getting-started/install/) å®˜æ–¹æ–‡ä»¶ç‚ºä¸»ã€‚


# 1 æº–å‚™

## 1.1 å‰ç½®ä½œæ¥­

æˆ‘ç”¨ VMWare æº–å‚™äº†ä¸€å° Ubuntu 20.04ï¼Œè³‡æºæœ‰:
- CPU: 4core
- Memory: 16GiB
- Disk: 100GiB
- Network: Bridge Mode

è»Ÿé«”æº–å‚™ï¼š
- Docker CE v20.x ä»¥ä¸Šï¼Œç›¸é—œå®‰è£å¯ä»¥åƒè€ƒ [K8s å­¸ç¿’ç­†è¨˜ - kubeadm æ‰‹å‹•å®‰è£](https://rickhw.github.io/2019/03/17/Container/Install-K8s-with-Kubeadm/)

å¦‚æœæ˜¯ç›´æ¥åœ¨ macOS ä¸Šç”¨ Docker CE è·‘ï¼Œå»ºè­°å…ˆä¿®æ”¹ docker Resources è³‡æºï¼Œæˆ‘æŠŠè¨˜æ†¶é«”æ‹‰åˆ°å¾ˆé«˜ï¼ŒåŒæ™‚é—œæ‰å…¶ä»–çš„ containersï¼Œå¦‚åœ–ï¼š


![](image/docker-ce-macos.png)

## 1.2 æº–å‚™ K8s & EKS CLI

EKS Anywhere ä¸»è¦ä½¿ç”¨ å…©å€‹ AWS çš„ CLI:

1. `eksctl`
2. `eksctl-anywhere`

ä¸‹è¼‰é€™å…©å€‹å·¥å…·ï¼Œé€™å€‹ç­†è¨˜çµ±ä¸€éƒ½æ”¾åœ¨ `$HOME/bin` åº•ä¸‹ã€‚

```
## 1. ä¸‹è¼‰ eksctl  
~# curl "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \  
    --silent --location \  
    | tar xz -C /usr/local/bin  
  
  
## 2. ä¸‹è¼‰ eksctl-anywhere, ä¸¦æŒ‡å®š EKSA çš„ç‰ˆæœ¬  
~# export EKSA_RELEASE="0.5.0" OS="$(uname -s | tr A-Z a-z)"  
~# curl "https://anywhere-assets.eks.amazonaws.com/releases/eks-a/1/artifacts/eks-a/v${EKSA_RELEASE}/${OS}/eksctl-anywhere-v${EKSA_RELEASE}-${OS}-amd64.tar.gz" \  
    --silent --location \  
    | tar xz /usr/local/eksctl-anywhere  
  
~# eksctl anywhere version  
v0.5.0
```


æ–°çš„ VM ä¹Ÿè¦æº–å‚™ kubectlã€‚

## 1.3 ä¸‹è¼‰ docker images

EKS anywhere å®‰è£éç¨‹ï¼Œéœ€è¦ä½¿ç”¨ä¸€äº› EKS å®˜æ–¹çš„ Imagesï¼Œå¦‚æœç¶²è·¯é€Ÿåº¦ä¸å¿«ï¼Œå®‰è£éç¨‹éœ€è¦æ™‚é–“ç­‰å¾…ï¼Œæ‰€ä»¥å»ºè­°å…ˆæŠŠé€™äº› images ä¸‹è¼‰ã€‚

æˆ‘å®‰è£çš„æ™‚å€™ï¼Œä¸‹è¼‰äº†ä»¥ä¸‹ images:
```
root@ubuntu-server:~# docker images
REPOSITORY                                                                                        TAG                            IMAGE ID       CREATED        SIZE
public.ecr.aws/eks-anywhere/mrajashree/etcdadm-bootstrap-provider                                 v0.1.0-beta-4.1-eks-a-1        fdf90a12059b   13 days ago    39.5MB
public.ecr.aws/eks-anywhere/mrajashree/etcdadm-controller                                         v0.1.0-beta-4.1-eks-a-1        1df9b1e09c08   13 days ago    44.2MB
public.ecr.aws/eks-anywhere/cli-tools                                                             v0.1.0-eks-a-1                 ab7c32596071   13 days ago    390MB
public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node                                             v1.21.2-eks-d-1-21-4-eks-a-1   e786b2049245   13 days ago    1.64GB
public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api-provider-aws/cluster-api-aws-controller   v0.6.4-eks-a-1                 f19480314c0c   13 days ago    65.2MB
public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/capd-manager                              v0.3.23-eks-a-1                9551d1a8c5e4   13 days ago    228MB
public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/kubeadm-control-plane-controller          v0.3.23-eks-a-1                9e78bc6ef020   13 days ago    62.1MB
public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/kubeadm-bootstrap-controller              v0.3.23-eks-a-1                c74f411ef1c2   13 days ago    56.4MB
public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/cluster-api-controller                    v0.3.23-eks-a-1                8cc018325da7   13 days ago    57.4MB
public.ecr.aws/eks-anywhere/jetstack/cert-manager-webhook                                         v1.1.0-eks-a-1                 ff993602e2ca   13 days ago    42.4MB
public.ecr.aws/eks-anywhere/jetstack/cert-manager-controller                                      v1.1.0-eks-a-1                 07ed0e004245   13 days ago    52.8MB
public.ecr.aws/eks-anywhere/jetstack/cert-manager-cainjector                                      v1.1.0-eks-a-1                 3e6a2a6e0bd3   13 days ago    40.8MB
public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api-provider-vsphere/release/manager          v0.7.10-eks-a-1                a11c8cf5aa7f   13 days ago    55.4MB
public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy                                                v0.8.0-eks-a-1                 f35c58b91b9c   13 days ago    41.7MB
kindest/haproxy                                                                                   v20210715-a6da3463             083ad526a17e   2 months ago   31.8MB
hello-world    
```


åº•ä¸‹æ˜¯ç”¨ docker ä¸‹è¼‰çš„ scriptsï¼Œé€™äº›è³‡è¨Šæ˜¯åœ¨ `~/${CLUSTER_NAME}/generated/clusterctl_tmp.yaml` è£¡é¢æ‰¾åˆ°çš„ï¼Œä¸¦ä¸”æ•´ç†å‡ºä¾†çš„ï¼Œé€™äº›è³‡è¨Šæ‡‰è©²æœƒéš¨è‘—ç‰ˆæœ¬çš„æ¼”é€²è€Œæœ‰æ‰€æ”¹è®Šï¼Œæ‰€ä»¥ä»¥ä¸‹çš„è³‡è¨Šèˆ‡ç‰ˆæœ¬åƒ…ä¾›åƒè€ƒï¼š

```
docker pull public.ecr.aws/eks-anywhere/cli-tools:v0.1.0-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node:v1.21.2-eks-d-1-21-4-eks-a-1  
docker pull kindest/haproxy:v20210715-a6da3463  
  
docker pull public.ecr.aws/eks-anywhere/jetstack/cert-manager-cainjector:v1.1.0-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/jetstack/cert-manager-controller:v1.1.0-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/jetstack/cert-manager-webhook:v1.1.0-eks-a-1  
  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/cluster-api-controller:v0.3.23-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/kubeadm-bootstrap-controller:v0.3.23-eks-a-1  
  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/kubeadm-control-plane-controller:v0.3.23-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api-provider-aws/cluster-api-aws-controller:v0.6.4-eks-a-1  
  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1 #org one is v0.4.1  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api-provider-vsphere/release/manager:v0.7.10-eks-a-1  
  
  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1 #org one is v0.8.0  
docker pull public.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/capd-manager:v0.3.23-eks-a-1  
  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1 #org one is v0.4.0  
docker pull public.ecr.aws/eks-anywhere/mrajashree/etcdadm-bootstrap-provider:v0.1.0-beta-4.1-eks-a-1  
  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1 #org one is v0.4.0  
docker pull public.ecr.aws/eks-anywhere/mrajashree/etcdadm-controller:v0.1.0-beta-4.1-eks-a-1  
docker pull public.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy:v0.8.0-eks-a-1 #org one is v0.4.0
```

ä¸Šè¿°è³‡è¨Šæ˜¯æˆ‘è‡ªå·±æ‰‹å‹•æ•´ç†çš„ï¼Œæ‡‰è©²æœƒè®ŠåŒ–å¾ˆå¿«ã€‚ä¹‹æ‰€ä»¥æ•´ç†é€™ä»½ï¼Œæ˜¯æˆ‘åœ¨ lab çš„éç¨‹ï¼Œå‰›å¥½é‡åˆ°ç¶²è·¯ç‹€æ³ä¸å¥½ï¼Œæ‰€ä»¥èŠ±å¾ˆå¤šæ™‚é–“ä¸‹è¼‰ images ï¼Œå¦‚æœé å…ˆä¸‹è¼‰å¥½ï¼Œæ•´å€‹å®‰è£é€Ÿåº¦å°±å¿«å¾ˆå¤šã€‚






# 2 å»ºç«‹ Local Cluster

æº–å‚™å¥½å‰è¿°å·¥ä½œï¼Œæ¥ä¸‹ä¾†å°±å¯ä»¥é–‹å§‹å»ºç«‹ EKS Cluster äº†ã€‚


## 2.1 ç”¢ç”Ÿé…ç½®

é€é `eksctl` ç”¢ç”Ÿä¸€å€‹ clusterconfig:
```
export CLUSTER_NAME="ricklab-cluster"
eksctl anywhere generate clusterconfig $CLUSTER_NAME \
   --provider docker > $CLUSTER_NAME.yaml
```


çœ‹ä¸€ä¸‹é€™å€‹æª”æ¡ˆå…§å®¹æœ‰ä»€éº¼ï¼š
```
apiVersion: anywhere.eks.amazonaws.com/v1alpha1  
kind: Cluster  
metadata:  
  name: ricklab-cluster  
spec:  
  clusterNetwork:  
    cni: cilium  
    pods:  
      cidrBlocks:  
      - 172.16.0.0/16  
    services:  
      cidrBlocks:  
      - 10.96.0.0/12  
  controlPlaneConfiguration:  
    count: 1  
  datacenterRef:  
    kind: DockerDatacenterConfig  
    name: ricklab-cluster  
  externalEtcdConfiguration:  
    count: 1  
  kubernetesVersion: "1.21"  
  workerNodeGroupConfigurations:  
    - count: 1  
  
---  
apiVersion: anywhere.eks.amazonaws.com/v1alpha1  
kind: DockerDatacenterConfig  
metadata:  
  name: ricklab-cluster  
spec: {}  
  
---
```


é€™ä»½ config è£¡æœ‰å¹¾å€‹é‡è¦çš„è³‡è¨Šï¼š

1. K8s ç‰ˆæœ¬æ˜¯ `1.21`
    1. Control Plane æœ‰ä¸€å€‹ node
    2. Worker Node æœ‰ä¸€å€‹ node
    3. etcd æœ‰ä¸€å€‹ node
2. CNI ä½¿ç”¨ [cilium](https://cilium.io/) : eBPF-based Networking, Observability, and Security
    1. æŒ‡å®šäº† pod çš„ CIDR Blocks, æ³¨æ„é è¨­çš„ç¯„åœ `192.168.0.0/16`ï¼Œå¯èƒ½æœƒè·Ÿç¶²è·¯ç’°å¢ƒè¡çªæˆ–è€…é‡ç–Šï¼Œæ‰€ä»¥æˆ‘æ”¹æˆäº† `172.16.0.0/16`
    2. æŒ‡å®šäº† Services çš„ CIDR Blocksï¼Œè·Ÿæˆ‘çš„ç’°å¢ƒæ²’æœ‰è¡çªï¼Œæ‰€ä»¥æ²’æœ‰æ”¹ã€‚
3. Data Center çš„ Kind æ˜¯ Docker

## 2.2 å»ºç«‹ Local Cluster

å»ºç«‹ä¸€å€‹ Local Clusterï¼Œé€™æœƒèŠ±ä¸€é»æ™‚é–“

```
â¯ ~ Â·Â·Â· at 14:51:27
â¯ eksctl anywhere create cluster -f $CLUSTER_NAME.yaml -v 5
Performing setup and validations
Warning: The docker infrastructure provider is meant for local development and testing only
âœ… Docker Provider setup is valid
Creating new bootstrap cluster
Installing cluster-api providers on bootstrap cluster
Provider specific setup
Creating new workload cluster
Installing networking on workload cluster
Installing storage class on workload cluster
Installing cluster-api providers on workload cluster
Moving cluster management from bootstrap to workload cluster
Installing EKS-A custom components (CRD and controller) on workload cluster
Creating EKS-A CRDs instances on workload cluster
Installing AddonManager and GitOps Toolkit on workload cluster
GitOps field not specified, bootstrap flux skipped
Writing cluster config file
Deleting bootstrap cluster
ğŸ‰ Cluster created!

â¯  ~ Â·Â·Â· took 10m 11s at 15:02:00
â¯

```

ä¸Šè¿°è³‡è¨Šæ˜¯å®Œå…¨æ²’æœ‰èª¿æ•´ docker CE çš„è³‡æº (é è¨­è¨˜æ†¶é«”æ˜¯ 8GiB) ã€è·‘åœ¨ MacOS ä¸Šï¼Œéç¨‹è·‘äº†å°‡è¿‘ 10 åˆ†é˜ï¼Œèª¿æ•´åˆ° 20GiB ä¹‹å¾Œï¼Œè·‘å®Œç´„ 7 åˆ†é˜ã€‚

å¦‚æœç•«é¢å¾ˆä¹…æ²’åæ‡‰ï¼Œå¯ä»¥å¢åŠ åƒæ•¸ `-v 5` äº†è§£ç‹€æ³ï¼Œåº•ä¸‹æ˜¯åœ¨ Ubuntu ä¸ŠåŸ·è¡Œæ™‚çš„ç´€éŒ„ï¼š
```
root@ubuntu-server:~# export CLUSTER_NAME="dev-cluster"  
root@ubuntu-server:~# eksctl anywhere generate clusterconfig $CLUSTER_NAME \  
>    --provider docker > $CLUSTER_NAME.yaml  
  
root@ubuntu-server:~# eksctl anywhere create cluster -f $CLUSTER_NAME.yaml -v 5  
2021-09-23T13:12:57.401Z	V4	Logger init completed	{"vlevel": 5}  
2021-09-23T13:13:00.159Z	V1	Setting up cli docker dependencies  
2021-09-23T13:13:00.159Z	V2	Pulling docker image	{"image": "public.ecr.aws/eks-anywhere/cli-tools:v0.1.0-eks-a-1"}  
2021-09-23T13:13:02.407Z	V4	Task start	{"task_name": "setup-validate"}  
2021-09-23T13:13:02.407Z	V0	Performing setup and validations  
2021-09-23T13:13:02.407Z	V0	Warning: The docker infrastructure provider is meant for local development and testing only  
2021-09-23T13:13:02.407Z	V0	âœ… Docker Provider setup is valid  
2021-09-23T13:13:02.407Z	V4	Task finished	{"task_name": "setup-validate", "duration": "47.271Âµs"}  
2021-09-23T13:13:02.407Z	V4	----------------------------------  
2021-09-23T13:13:02.407Z	V4	Task start	{"task_name": "bootstrap-cluster-init"}  
2021-09-23T13:13:02.407Z	V0	Creating new bootstrap cluster  
2021-09-23T13:13:02.407Z	V4	Creating kind cluster	{"name": "ricklab-cluster-eks-a-cluster", "kubeconfig": "ricklab-cluster/generated/ricklab-cluster.kind.kubeconfig"}  
2021-09-23T13:13:47.919Z	V4	Applying extra objects	{"cluster": "ricklab-cluster", "resources": ["core-dns-clusterrole"]}  
2021-09-23T13:13:48.319Z	V0	Installing cluster-api providers on bootstrap cluster  
2021-09-23T13:16:06.334Z	V0	Provider specific setup  
2021-09-23T13:16:06.334Z	V4	Task finished	{"task_name": "bootstrap-cluster-init", "duration": "3m3.927762619s"}  
2021-09-23T13:16:06.334Z	V4	----------------------------------  
2021-09-23T13:16:06.334Z	V4	Task start	{"task_name": "workload-cluster-init"}  
2021-09-23T13:16:06.335Z	V0	Creating new workload cluster  
2021-09-23T13:16:07.096Z	V5	Retry execution successful	{"retries": 1, "duration": "760.594964ms"}  
2021-09-23T13:16:07.096Z	V3	Waiting for external etcd to be ready  
2021-09-23T13:16:22.806Z	V3	External etcd is ready  
2021-09-23T13:16:22.806Z	V3	Waiting for control plane to be ready  
2021-09-23T13:17:16.617Z	V5	Retry execution successful	{"retries": 1, "duration": "278.043187ms"}  
2021-09-23T13:17:16.617Z	V3	Waiting for controlplane and worker machines to be ready  
2021-09-23T13:17:16.850Z	V4	Nodes are not ready yet	{"total": 2, "ready": 1}  
2021-09-23T13:17:17.090Z	V4	Nodes are not ready yet	{"total": 2, "ready": 1}  
2021-09-23T13:17:17.090Z	V5	Error happened during retry	{"error": "nodes are not ready yet", "retries": 1}  
2021-09-23T13:17:17.090Z	V5	Sleeping before next retry	{"time": "1s"}  
2021-09-23T13:17:18.331Z	V4	Nodes are not ready yet	{"total": 2, "ready": 1}  
2021-09-23T13:17:18.331Z	V5	Error happened during retry	{"error": "nodes are not ready yet", "retries": 2}  
2021-09-23T13:17:18.331Z	V5	Sleeping before next retry	{"time": "1s"}  
  
... ç•¥ ...  
  
2021-09-23T13:18:10.382Z	V0	Installing storage class on workload cluster  
2021-09-23T13:18:10.382Z	V0	Installing cluster-api providers on workload cluster  
2021-09-23T13:20:40.502Z	V4	Installing machine health checks on bootstrap cluster  
2021-09-23T13:20:40.502Z	V4	Skipping machine health checks  
2021-09-23T13:20:40.502Z	V4	Task finished	{"task_name": "workload-cluster-init", "duration": "4m34.167905829s"}  
2021-09-23T13:20:40.502Z	V4	----------------------------------  
2021-09-23T13:20:40.502Z	V4	Task start	{"task_name": "capi-management-move"}  
2021-09-23T13:20:40.502Z	V0	Moving cluster management from bootstrap to workload cluster  
2021-09-23T13:20:40.502Z	V3	Waiting for management machines to be ready before move  
2021-09-23T13:20:40.774Z	V4	Nodes ready	{"total": 2}  
2021-09-23T13:20:43.420Z	V3	Waiting for control planes to be ready after move  
2021-09-23T13:20:44.938Z	V3	Waiting for machines to be ready after move  
2021-09-23T13:20:45.239Z	V4	Nodes ready	{"total": 2}  
2021-09-23T13:20:45.239Z	V4	Task finished	{"task_name": "capi-management-move", "duration": "4.736253314s"}  
2021-09-23T13:20:45.239Z	V4	----------------------------------  
2021-09-23T13:20:45.239Z	V4	Task start	{"task_name": "eksa-components-install"}  
2021-09-23T13:20:45.239Z	V0	Installing EKS-A custom components (CRD and controller) on workload cluster  
2021-09-23T13:20:46.961Z	V5	Retry execution successful	{"retries": 1, "duration": "962.076365ms"}  
2021-09-23T13:21:06.898Z	V0	Creating EKS-A CRDs instances on workload cluster  
2021-09-23T13:21:06.898Z	V4	Applying eksa yaml resources to cluster  
2021-09-23T13:21:07.608Z	V5	Retry execution successful	{"retries": 1, "duration": "710.107178ms"}  
2021-09-23T13:21:08.015Z	V5	Retry execution successful	{"retries": 1, "duration": "403.533216ms"}  
2021-09-23T13:21:08.340Z	V5	Retry execution successful	{"retries": 1, "duration": "324.587963ms"}  
2021-09-23T13:21:08.686Z	V5	Retry execution successful	{"retries": 1, "duration": "346.697543ms"}  
2021-09-23T13:21:08.686Z	V4	Task finished	{"task_name": "eksa-components-install", "duration": "23.447764425s"}  
2021-09-23T13:21:08.686Z	V4	----------------------------------  
2021-09-23T13:21:08.687Z	V4	Task start	{"task_name": "addon-manager-install"}  
2021-09-23T13:21:08.687Z	V0	Installing AddonManager and GitOps Toolkit on workload cluster  
2021-09-23T13:21:08.687Z	V0	GitOps field not specified, bootstrap flux skipped  
2021-09-23T13:21:08.687Z	V4	Task finished	{"task_name": "addon-manager-install", "duration": "16.609Âµs"}  
2021-09-23T13:21:08.687Z	V4	----------------------------------  
2021-09-23T13:21:08.687Z	V4	Task start	{"task_name": "write-cluster-config"}  
2021-09-23T13:21:08.687Z	V0	Writing cluster config file  
2021-09-23T13:21:08.687Z	V4	Task finished	{"task_name": "write-cluster-config", "duration": "381.756Âµs"}  
2021-09-23T13:21:08.687Z	V4	----------------------------------  
2021-09-23T13:21:08.687Z	V4	Task start	{"task_name": "delete-kind-cluster"}  
2021-09-23T13:21:08.687Z	V0	Deleting bootstrap cluster  
2021-09-23T13:21:08.944Z	V5	Executed kind get clusters	{"response": "dev-cluster\ndev-cluster-eks-a-cluster\nricklab-cluster\nricklab-cluster-eks-a-cluster\n"}  
2021-09-23T13:21:09.549Z	V4	Deleting kind cluster	{"name": "ricklab-cluster-eks-a-cluster"}  
2021-09-23T13:21:10.918Z	V0	ğŸ‰ Cluster created!  
2021-09-23T13:21:10.918Z	V4	Task finished	{"task_name": "delete-kind-cluster", "duration": "2.230882453s"}  
2021-09-23T13:21:10.918Z	V4	----------------------------------  
2021-09-23T13:21:10.918Z	V4	Tasks completed	{"duration": "8m8.511328006s"}  
root@ubuntu-server:~#
```

é€™æ®µ log å¯ä»¥çœ‹åˆ°éç¨‹è©³ç´°çš„åŸ·è¡Œè³‡è¨Šï¼Œæ•´å€‹éç¨‹è·‘äº†å¤§ç´„ 8 åˆ†é˜ï¼Œé€™æ˜¯æˆ‘å·²ç¶“é å…ˆä¸‹è¼‰ docker images çš„ç‹€æ³ï¼Œæ‰€ä»¥å»ºè­°å…ˆä¸‹è¼‰é docekr images.

> åŒæ¨£çš„èƒŒæ™¯ï¼Œæœ‰ä¸€æ¬¡è£è·‘äº† 25m ï¼ŒåŒæ¨£æ˜¯ docker images é å…ˆä¸‹è¼‰å¥½çš„ç‹€æ³ã€‚

## 2.3 è§€å¯Ÿ

å¾å‰è¿°çš„ detail log å¯ä»¥ç™¼ç¾æœ‰ä»¥ä¸‹ tasks:
1. setup-validate
2. bootstrap-cluster-init
3. workload-cluster-init
4. capi-management-move
5. eksa-components-install
6. addon-manager-install
7. write-cluster-config
8. delete-kind-cluster


æ¯å€‹æ­¥é©ŸèƒŒå¾Œéƒ½æ˜¯é€é container `public.ecr.aws/eks-anywhere/cli-tools:v0.1.0-eks-a-1` åœ¨æ§åˆ¶èˆ‡æ“ä½œï¼Œåº•ä¸‹æ˜¯ `bootstrap-cluster-init` çš„éç¨‹ï¼š
```
root@ubuntu-server:~# docker logs inspiring_chebyshev
Creating cluster "ricklab-cluster-eks-a-cluster" ...
 â€¢ Ensuring node image (public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node:v1.21.2-eks-d-1-21-4-eks-a-1) ğŸ–¼  ...
 âœ“ Ensuring node image (public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node:v1.21.2-eks-d-1-21-4-eks-a-1) ğŸ–¼
 â€¢ Preparing nodes ğŸ“¦   ...
 âœ“ Preparing nodes ğŸ“¦
 â€¢ Writing configuration ğŸ“œ  ...
 âœ“ Writing configuration ğŸ“œ
 â€¢ Starting control-plane ğŸ•¹ï¸  ...
 âœ“ Starting control-plane ğŸ•¹ï¸
 â€¢ Installing CNI ğŸ”Œ  ...
 âœ“ Installing CNI ğŸ”Œ
 â€¢ Installing StorageClass ğŸ’¾  ...
 âœ“ Installing StorageClass ğŸ’¾
Set kubectl context to "kind-ricklab-cluster-eks-a-cluster"
You can now use your cluster with:

kubectl cluster-info --context kind-ricklab-cluster-eks-a-cluster --kubeconfig ricklab-cluster/generated/ricklab-cluster.kind.kubeconfig

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community ğŸ™‚
root@ubuntu-server:~#
```



éç¨‹ä¸­æ¯”è¼ƒèŠ±æ™‚é–“çš„æ˜¯é€™å€‹ Waiting for cert-manager to be available çš„ taskï¼Œéç¨‹åœ¨ç­‰å¾…å•¥æ±è¥¿ï¼Œæˆ‘æ²’æ‰¾åˆ°åŸå› ï¼ŒGithub ä¸Šçš„ Issue ç›®å‰ (2021/09/23) é‚„æ˜¯ Open çš„ç‹€æ…‹ã€‚ã€‚ã€‚ä»–çš„ Log å¦‚ä¸‹ï¼š
```
root@ubuntu-server:~# docker ps  
CONTAINER ID   IMAGE                                                                                COMMAND                  CREATED              STATUS              PORTS                       NAMES  
66edc1f9a3d9   public.ecr.aws/eks-anywhere/cli-tools:v0.1.0-eks-a-1                                 "clusterctl init --câ€¦"   38 seconds ago       Up 37 seconds                                   serene_hopper  
57932fdcc915   public..../node:v1.21.2-eks-d-1-21-4-eks-a-1   "/usr/local/bin/entrâ€¦"   About a minute ago   Up About a minute   127.0.0.1:42309->6443/tcp   dev-cluster-eks-a-cluster-control-plane  
  
root@ubuntu-server:~# docker logs -f serene_hopper  
Fetching providers  
Using Override="core-components.yaml" Provider="cluster-api" Version="v0.3.23"  
Using Override="bootstrap-components.yaml" Provider="bootstrap-kubeadm" Version="v0.3.23"  
Using Override="bootstrap-components.yaml" Provider="bootstrap-etcdadm-bootstrap" Version="v0.1.0-beta-4.1"  
Using Override="bootstrap-components.yaml" Provider="bootstrap-etcdadm-controller" Version="v0.1.0-beta-4.1"  
Using Override="control-plane-components.yaml" Provider="control-plane-kubeadm" Version="v0.3.23"  
Using Override="infrastructure-components-development.yaml" Provider="infrastructure-docker" Version="v0.3.23"  
Installing cert-manager Version="v1.1.0"  
Waiting for cert-manager to be available...  
Installing Provider="cluster-api" Version="v0.3.23" TargetNamespace="capi-system"  
Installing Provider="bootstrap-kubeadm" Version="v0.3.23" TargetNamespace="capi-kubeadm-bootstrap-system"  
Installing Provider="bootstrap-etcdadm-bootstrap" Version="v0.1.0-beta-4.1" TargetNamespace="etcdadm-bootstrap-provider-system"  
Installing Provider="bootstrap-etcdadm-controller" Version="v0.1.0-beta-4.1" TargetNamespace="etcdadm-controller-system"  
Installing Provider="control-plane-kubeadm" Version="v0.3.23" TargetNamespace="capi-kubeadm-control-plane-system"  
Installing Provider="infrastructure-docker" Version="v0.3.23" TargetNamespace="capd-system"  
  
Your management cluster has been initialized successfully!  
  
You can now create your first workload cluster by running the following:  
  
  clusterctl config cluster [name] --kubernetes-version [version] | kubectl apply -f -  
  
root@ubuntu-server:~#
```



åº•ä¸‹æ˜¯å…¶ä»– task çš„ logs è§€å¯Ÿï¼Œå› ç‚º container name æ²’æœ‰ç”¨ task name ï¼Œæ‰€ä»¥çœ‹ä¸å‡ºä¾†æ˜¯å“ªå€‹ taskï¼š
```
root@ubuntu-server:~# docker logs -f cool_mcclintock  
deployment.apps/capi-kubeadm-bootstrap-controller-manager condition met  
  
root@ubuntu-server:~# docker logs -f zealous_mclaren  
deployment.apps/capd-controller-manager condition met  
  
root@ubuntu-server:~# docker logs -f cranky_bouman  
cluster.cluster.x-k8s.io/dev-cluster condition met  
  
root@ubuntu-server:~# docker logs -f cranky_hofstadter  
cluster.cluster.x-k8s.io/dev-cluster condition met
```




æœ€å¾ŒæŸ¥çœ‹ etcd çš„ Logsï¼š
```
root@ubuntu-server:~# docker logs ricklab-cluster-etcd-cjvzj  
INFO: ensuring we can execute mount/umount even with userns-remap  
INFO: remounting /sys read-only  
INFO: making mounts shared  
INFO: detected cgroup v1  
INFO: fix cgroup mounts for all subsystems  
INFO: clearing and regenerating /etc/machine-id  
Initializing machine ID from random generator.  
INFO: faking /sys/class/dmi/id/product_name to be "kind"  
INFO: faking /sys/class/dmi/id/product_uuid to be random  
INFO: faking /sys/devices/virtual/dmi/id/product_uuid as well  
INFO: setting iptables to detected mode: legacy  
INFO: Detected IPv4 address: 172.18.0.4  
INFO: Detected IPv6 address: fc00:f853:ccd:e793::4  
  
Welcome to Amazon Linux 2!  
  
[  OK  ] Set up automount Arbitrary Executab...ats File System Automount Point.  
[  OK  ] Reached target Local File Systems.  
[  OK  ] Reached target Swap.  
[  OK  ] Started Dispatch Password Requests to Console Directory Watch.  
[  OK  ] Reached target Paths.  
[  OK  ] Created slice Root Slice.  
  
... ç•¥ ...  
  
         Starting Create Static Device Nodes in /dev...  
         Starting Flush Journal to Persistent Storage...  
[  OK  ] Started Create Static Device Nodes in /dev.  
[  OK  ] Started Update UTMP about System Boot/Shutdown.  
[  OK  ] Started Flush Journal to Persistent Storage.  
[  OK  ] Started Rebuild Hardware Database.  
         Starting Update is Completed...  
[  OK  ] Started Update is Completed.  
[  OK  ] Reached target System Initialization.  
[  OK  ] Started Daily Cleanup of Temporary Directories.  
[  OK  ] Reached target Timers.  
[  OK  ] Listening on D-Bus System Message Bus Socket.  
[  OK  ] Reached target Sockets.  
[  OK  ] Reached target Basic System.  
         Starting containerd container runtime...  
         Starting Cleanup of Temporary Directories...  
[  OK  ] Started Cleanup of Temporary Directories.  
[  OK  ] Started containerd container runtime.  
[  OK  ] Reached target Multi-User System.  
[  OK  ] Reached target Graphical Interface.  
         Starting Update UTMP about System Runlevel Changes...  
[  OK  ] Started Update UTMP about System Runlevel Changes.
```



æœ€å¾Œè§€å¯Ÿ docker ps å»ºç«‹é‚£ä¸€äº› container:
```
root@ubuntu-server:~# docker ps
CONTAINER ID   IMAGE                                                                                COMMAND                  CREATED          STATUS          PORTS                                  NAMES
2e432b89f3f8   public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node:v1.21.2-eks-d-1-21-4-eks-a-1   "/usr/local/bin/entrâ€¦"   13 minutes ago   Up 13 minutes                                          ricklab-cluster-md-0-76b7b5578c-zhs7d
d0e7ac20a689   public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node:v1.21.2-eks-d-1-21-4-eks-a-1   "/usr/local/bin/entrâ€¦"   14 minutes ago   Up 14 minutes   34917/tcp, 127.0.0.1:34917->6443/tcp   ricklab-cluster-4n42d
a18287b7d33d   public.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node:v1.21.2-eks-d-1-21-4-eks-a-1   "/usr/local/bin/entrâ€¦"   14 minutes ago   Up 14 minutes                                          ricklab-cluster-etcd-cjvzj
7c95c9a2a15a   kindest/haproxy:v20210715-a6da3463                                                   "haproxy -sf 7 -W -dâ€¦"   14 minutes ago   Up 14 minutes   45713/tcp, 0.0.0.0:45713->6443/tcp     ricklab-cluster-lb
root@ubuntu-server:~#
```

è£¡é¢åŒ…å«äº†:
- `control plane`: ricklab-cluster-4n42d
- `worker node`: ricklab-cluster-md-0-76b7b5578c-zhs7d
- `etced`: ricklab-cluster-etcd-cjvzj
- `load balance`: haproxy -> å¯¦éš›ä¸Šä¸¦ä¸æ˜¯ ingress controller.


## 2.4 é©—è­‰ Cluster

ç¢ºèª cluster æ˜¯å¦æ­£å¸¸é‹ä½œ

- CRI: containerd
- OS-Image: Amazon Linux 2
- CNI: cilium

```
export CLUSTER_NAME="ricklab-cluster"  
export KUBECONFIG=${HOME}/${CLUSTER_NAME}/${CLUSTER_NAME}-eks-a-cluster.kubeconfig  
  
# å–å¾— componentstatus (cs)  
root@ubuntu-server:~# kubectl get cs  
Warning: v1 ComponentStatus is deprecated in v1.19+  
NAME                 STATUS      MESSAGE                                                                                       ERROR  
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused  
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused  
etcd-0               Healthy     {"health":"true"}  
  
## å¦‚æœç‹€æ…‹è·Ÿä¸Šé¢ä¸€æ¨£æœ‰å•é¡Œï¼Œè«‹åƒé–±é€™ä»½æ–‡ä»¶èª¿æ•´:  https://my.oschina.net/u/1431757/blog/4550843  
## æ­£å¸¸å¦‚ä¸‹ï¼š  
root@ubuntu-server:~# kubectl get cs  
Warning: v1 ComponentStatus is deprecated in v1.19+  
NAME                 STATUS    MESSAGE             ERROR  
controller-manager   Healthy   ok  
etcd-0               Healthy   {"health":"true"}  
scheduler            Healthy   ok  
  
  
# å–çš„ api-resources  
root@ubuntu-server:~# kubectl api-resources  
NAME                              SHORTNAMES     APIVERSION                                     NAMESPACED   KIND  
bindings                                         v1                                             true         Binding  
componentstatuses                 cs             v1                                             false        ComponentStatus  
configmaps                        cm             v1                                             true         ConfigMap  
endpoints                         ep             v1                                             true         Endpoints  
  
... ç•¥ ...  
  
namespaces                        ns             v1                                             false        Namespace  
nodes                             no             v1                                             false        Node  
persistentvolumeclaims            pvc            v1                                             true         PersistentVolumeClaim  
  
... ç•¥ ...  
  
# å–å¾—ç›®å‰çš„ namespaces  
root@ubuntu-server:~# kubectl get ns  
NAME                                STATUS   AGE  
capd-system                         Active   17m  
capi-kubeadm-bootstrap-system       Active   17m  
capi-kubeadm-control-plane-system   Active   17m  
capi-system                         Active   17m  
capi-webhook-system                 Active   17m  
cert-manager                        Active   18m  
default                             Active   19m  
eksa-system                         Active   16m  
etcdadm-bootstrap-provider-system   Active   17m  
etcdadm-controller-system           Active   17m  
kube-node-lease                     Active   19m  
kube-public                         Active   19m  
kube-system                         Active   19m  
root@ubuntu-server:~#  
  
# å–å¾—ç›®å‰çš„ nodes ç‹€æ…‹  
root@ubuntu-server:~# kubectl get node -o wide  
NAME                                    STATUS   ROLES                  AGE   VERSION              INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION     CONTAINER-RUNTIME  
ricklab-cluster-4n42d                   Ready    control-plane,master   17m   v1.21.2-eks-1-21-4   172.18.0.5    <none>        Amazon Linux 2   5.4.0-84-generic   containerd://1.4.6  
ricklab-cluster-md-0-76b7b5578c-zhs7d   Ready    <none>                 16m   v1.21.2-eks-1-21-4   172.18.0.6    <none>        Amazon Linux 2   5.4.0-84-generic   containerd://1.4.6  
root@ubuntu-server:~#  
  
  
# å–çš„ pod  
root@ubuntu-server:~# kubectl get pod --all-namespaces  
NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE  
capd-system                         capd-controller-manager-659dd5f8bc-bnp5m                         2/2     Running   0          15m  
capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-69889cb844-ndxsp       2/2     Running   0          15m  
capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-6ddc66fb75-mhmn7   2/2     Running   0          15m  
capi-system                         capi-controller-manager-db59f5789-w9dvp                          2/2     Running   0          15m  
capi-webhook-system                 capi-controller-manager-64b8c548db-wv6mz                         2/2     Running   0          15m  
capi-webhook-system                 capi-kubeadm-bootstrap-controller-manager-68b8cc9759-75l4p       2/2     Running   0          15m  
capi-webhook-system                 capi-kubeadm-control-plane-controller-manager-7dc88f767d-7cx4l   2/2     Running   0          15m  
cert-manager                        cert-manager-5f6b885b4-gcf95                                     1/1     Running   0          16m  
cert-manager                        cert-manager-cainjector-bb6d9bcb5-rrp7b                          1/1     Running   0          16m  
cert-manager                        cert-manager-webhook-56cbc8f5b8-2767z                            1/1     Running   0          16m  
eksa-system                         eksa-controller-manager-6769764b45-jv5q9                         2/2     Running   0          14m  
etcdadm-bootstrap-provider-system   etcdadm-bootstrap-provider-controller-manager-54476b7bf9-qgblp   2/2     Running   0          15m  
etcdadm-controller-system           etcdadm-controller-controller-manager-d5795556-bxsb2             2/2     Running   0          15m  
kube-system                         cilium-7gkp5                                                     1/1     Running   0          16m  
kube-system                         cilium-bq8pk                                                     1/1     Running   0          16m  
kube-system                         cilium-operator-6bf46cc6c6-56wn2                                 1/1     Running   0          16m  
kube-system                         cilium-operator-6bf46cc6c6-lkxrn                                 1/1     Running   0          16m  
kube-system                         coredns-7c68f85774-2pnxk                                         1/1     Running   0          17m  
kube-system                         coredns-7c68f85774-m7f99                                         1/1     Running   0          17m  
kube-system                         kube-apiserver-ricklab-cluster-4n42d                             1/1     Running   0          17m  
kube-system                         kube-controller-manager-ricklab-cluster-4n42d                    1/1     Running   0          17m  
kube-system                         kube-proxy-mnrtv                                                 1/1     Running   0          17m  
kube-system                         kube-proxy-sl6sq                                                 1/1     Running   0          16m  
kube-system                         kube-scheduler-ricklab-cluster-4n42d                             1/1     Running   0          17m  
root@ubuntu-server:~#
```



# 3 ç”¨ Rancher è§€å¯Ÿ

æŠŠ cluster åŒ¯å…¥ Rancher æŸ¥çœ‹ï¼Œåº•ä¸‹æ˜¯ Node å’Œ Pod çš„ç‹€æ³

![](image/rancher_nodes.png)

![](image/rancher_pod.png)


# 4 Deploy an application

éƒ¨ç½²å®˜æ–¹æä¾›çš„æ‡‰ç”¨ç¯„ä¾‹ï¼š

```
kubectl apply -f "https://anywhere.eks.amazonaws.com/manifests/hello-eks-a.yaml"  
  
kubectl get pods -l app=hello-eks-a  
  
root@ubuntu-server:~# kubectl logs -l app=hello-eks-a  
2021/09/21 15:07:07 [notice] 1#1: using the "epoll" event method  
2021/09/21 15:07:07 [notice] 1#1: nginx/1.21.1  
2021/09/21 15:07:07 [notice] 1#1: built by gcc 10.3.1 20210424 (Alpine 10.3.1_git20210424)  
2021/09/21 15:07:07 [notice] 1#1: OS: Linux 5.4.0-84-generic  
2021/09/21 15:07:07 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576  
2021/09/21 15:07:07 [notice] 1#1: start worker processes  
2021/09/21 15:07:07 [notice] 1#1: start worker process 39  
2021/09/21 15:07:07 [notice] 1#1: start worker process 40  
2021/09/21 15:07:07 [notice] 1#1: start worker process 41  
2021/09/21 15:07:07 [notice] 1#1: start worker process 42  
  
  
root@ubuntu-server:~# kubectl port-forward deploy/hello-eks-a 8000:80  
Forwarding from 127.0.0.1:8000 -> 80  
Forwarding from [::1]:8000 -> 80  
Handling connection for 8000  
  
  
## æ¸¬è©¦  
root@ubuntu-server:~# curl http://localhost:8000  
â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢  
  
Thank you for using  
  
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â•â•  
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â•šâ•â•â•â•â–ˆâ–ˆâ•‘  
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘  
â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•  
  
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•  
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘ â–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•   â•šâ•â•    â•šâ•â•â•â•šâ•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•  
  
You have successfully deployed the hello-eks-a pod hello-eks-a-9644dd8dc-fn4vk  
  
For more information check out  
https://anywhere.eks.amazonaws.com  
  
â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢â¬¡â¬¢  
root@ubuntu-server:~#****
```



# 5 åˆªé™¤ Cluster

æœ‰é ­æœ‰å°¾ï¼Œå®˜æ–¹æ–‡ä»¶èªªç”¨ä»¥ä¸‹åˆªé™¤ clsuter

```
export CLUSTER_NAME="ricklab-cluster"  
export KUBECONFIG=${HOME}/${CLUSTER_NAME}/${CLUSTER_NAME}-eks-a-cluster.kubeconfig  
  
eksctl anywhere delete cluster ${CLUSTER_NAME}  
  
Performing provider setup and validations  
Creating management cluster  
Installing cluster-api providers on management cluster  
Moving cluster management from workload cluster  
Deleting workload cluster  
Clean up Git Repo  
GitOps field not specified, clean up git repo skipped  
ğŸ‰ Cluster deleted!
```

æˆ‘å¯¦éš›ä¸Šéƒ½ç›´æ¥ç äº† docker container æ¯”è¼ƒå¿« XD



# 6 and A



- Q: Local Cluster å»ºè­°çš„ç¡¬é«”è³‡æºæ˜¯å¤šå°‘ï¼Ÿ

> åº•ä¸‹æ˜¯å®˜æ–¹çµ¦çš„è³‡è¨Šï¼š
> 
> - Docker 20.x.x
> - Mac OS (10.15) / Ubuntu (20.04.2 LTS)
> - 4 CPU cores
> - 16GB memory
> - 30GB free disk space
> 
> å¯¦éš›ä¸Šå‰‡çœ‹ä½¿ç”¨éœ€æ±‚è€Œè¨€ã€‚

- Q: ç‚ºä»€éº¼æœƒéœ€è¦ç”¨ EKS Anywhereï¼Ÿ

> é‡å°å¦‚æœè¦ä½¿ç”¨åƒæ˜¯ `IRSA` åš AuthNã€AuthZ ï¼Œæˆ–è€… `OIDC` ç®¡ç†å¸³è™Ÿï¼Œ
> 
> æˆ‘å€‹äººä¹‹æ‰€ä»¥æƒ³è©¦çœ‹çœ‹ EKS Anywhere ä¸»è¦æ˜¯å› ç‚ºå…¬å¸çš„ç’°å¢ƒéƒ½ä»¥ EKS ç‚ºä¸»ï¼Œè€Œæˆ‘åœ¨éƒ¨é–€å…§éƒ¨æœ‰ä¸€å° PC ç•¶ Lab ç’°å¢ƒï¼Œæˆ‘è£äº† Proxmoxï¼Œä¸Šé¢æˆ‘æ‰‹å‹•ç”¨ [kubeadm](https://rickhw.github.io/2019/03/17/Container/Install-K8s-with-Kubeadm/) è£äº†æ•´å€‹ç’°å¢ƒè®“åœ˜éšŠç·´ç¿’ä½¿ç”¨ã€‚é›–ç„¶æˆ‘ç®—æ˜¯å¾ˆç†Ÿç·´æ•´å€‹å®‰è£éç¨‹ï¼Œä½†å¦‚æœè¦äº¤çµ¦å…¶ä»–æˆå“¡ç¶­è­·çš„è©±ï¼Œé‚„æ˜¯æœ‰äº›é›£åº¦ã€‚å¦å¤–æˆ‘å€‘ä½¿ç”¨ EKS çš„ IRSA åšç®¡ç†ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿå¸Œæœ›èƒ½å¤ è®“ Lab ç’°å¢ƒè·Ÿç·šä¸Šç’°å¢ƒæ›´ä¸€è‡´ã€‚

- Q: å¦‚ä½•è®“å¤–é¢å­˜å– cluster?

> ç¯„ä¾‹ä¸­çš„ `${HOME}/${CLUSTER_NAME}/${CLUSTER_NAME}-eks-a-cluster.kubeconfig` è£¡é¢çš„ `server: https://127.0.0.1:43835` æ˜¯æœ¬æ©Ÿä½ç½®ï¼Œä¿®æ”¹æˆæ©Ÿå™¨ (host machine) çš„ IP å°±å¯ä»¥å¾å¤–é¢æ“ä½œäº†ã€‚

- Q: èª°é©åˆä½¿ç”¨ EKS Anywhereï¼Ÿ

> K8s çš„ä½¿ç”¨è€…æœ‰å…©å€‹é¢å‘ï¼š`1) ä½¿ç”¨å®¹å™¨`ã€`2) ç®¡ç†å®¹å™¨å¹³å°`ï¼Œé€™æ˜¯å…©å€‹ä¸åŒä½¿ç”¨è€…å°å‘çš„è¨­è¨ˆï¼Œæ›´å¤šåƒé–± [é›²åŸç”Ÿäººæ‰å°‹æ‰¾çš„é›£è™•åŠå¸¸è¦‹èª¤å€æ¢è¨, P12-13](https://speakerdeck.com/pichuang/20210824-yun-yuan-sheng-ren-cai-xun-zhao-de-nan-chu-ji-chang-jian-wu-qu-tan-tao?slide=12) ã€‚è€Œ EKS Anywhere æˆ‘è¦ºå¾—æ˜¯å¾Œè€…ï¼Œä¹Ÿå°±æ˜¯è¨­è¨ˆçµ¦ `ç®¡ç†å®¹å™¨å¹³å°` çš„äººã€‚è¨­è¨ˆçµ¦ä½¿ç”¨å®¹å™¨çš„ K8s å¹³å°ï¼Œå¤§æ¦‚å°±æ˜¯ minikubeã€microK8sã€K3d ä¹‹é¡çš„ã€‚









