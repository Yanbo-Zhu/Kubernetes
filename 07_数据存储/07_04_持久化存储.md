
# 1 Volume

## 1.1 基础

- Kubernetes 支持很多类型的卷。 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 可以同时使用任意数目的卷类型。
- 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。
- 当 Pod 不再存在时，Kubernetes 也会销毁临时卷。
- Kubernetes 不会销毁 `持久卷` 。
- 对于给定 Pod 中 `任何类型的卷` ，在容器重启期间数据都不会丢失。
- 使用卷时, 在 `.spec.volumes` 字段中设置为 Pod 提供的卷，并在 `.spec.containers[*].volumeMounts` 字段中声明卷在容器中的挂载位置。

## 1.2 subPath

- 有时，在单个 Pod 中共享卷以供多方使用是很有用的。 `volumeMounts.subPath` 属性可用于指定所引用的卷内的子路径，而不是其根路径。

注意：ConfigMap 和 Secret 使用子路径挂载是无法热更新的。


# 2 安装 NFS

- NFS 的简介：网络文件系统，英文Network File System(NFS)，是由 [SUN](https://baike.baidu.com/item/SUN/69463) 公司研制的[UNIX](https://baike.baidu.com/item/UNIX/219943)[表示层](https://baike.baidu.com/item/%E8%A1%A8%E7%A4%BA%E5%B1%82/4329716)协议(presentation layer protocol)，能使使用者访问网络上别处的文件就像在使用自己的计算机一样。

![](https://cdn.nlark.com/yuque/0/2022/png/513185/1648539022632-fd301755-4510-45e3-b047-a3a1e6916aff.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_36%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)


注意：实际开发中，不建议使用 NFS 作为 Kubernetes 集群持久化的驱动。


原因：

Network File System (NFS) 在某些情况下可能是一个很好的选择，特别是在小规模和简单的生产环境中。然而，对于一些更复杂、要求高可用性和性能的环境，NFS 可能存在一些挑战，因此在选择存储解决方案时需要权衡一些因素：

1. 单点故障： NFS 通常是基于服务器-客户端模型的，如果 NFS 服务器出现故障，整个存储系统可能变得不可用。在高可用性环境中，要确保 NFS 服务器的冗余性可能需要一些额外的配置。
2. 性能： NFS 的性能可能受到网络和服务器性能的限制。在某些高性能和低延迟要求的场景中，可能需要考虑其他分布式存储系统，比如 Ceph 或者使用云提供商的块存储服务。
3. 复杂性： 尽管 NFS 是一种相对简单的共享文件系统，但在一些复杂的部署中，特别是在大规模集群中，可能需要更灵活和可扩展的解决方案。一些企业可能更愿意选择分布式存储系统，如 Ceph 或 GlusterFS。
4. 权限和安全性： NFS 的安全性依赖于正确的配置。确保适当的权限和身份验证对于防止未经授权的访问至关重要。在一些安全性要求较高的场景中，可能需要考虑其他存储解决方案，如加密存储或者云提供商的加密选项。
5. 云原生： 对于在云中运行的 Kubernetes 集群，云提供商通常提供了更集成、更适应云原生应用的存储解决方案，这可能更容易管理和扩展。  


总的来说，NFS 是一个成熟且易于使用的存储协议，但在考虑在生产环境中选择存储解决方案时，需要仔细评估具体需求和限制。对于大规模、高可用性和高性能要求的环境，可能需要考虑更先进的存储解决方案。


## 2.1 例子 

https://www.yuque.com/fairy-era/yg511q/pyll1k#07c7e76e

### 2.1.1 前期准备 

本次以 Master （192.168.65.100）节点作为 NFS 服务端：
yum install -y nfs-utils

在 Master（192.168.65.100）节点创建 /etc/exports 文件
```
# * 表示暴露权限给所有主机；* 也可以使用 192.168.0.0/16 代替，表示暴露给所有主机
echo "/nfs/data/ *(insecure,rw,sync,no_root_squash)" > /etc/exports
```


在 Master（192.168.65.100）节点创建 /nfs/data/ （共享目录）目录，并设置权限：
```
mkdir -pv /nfs/data/
chmod 777 -R /nfs/data/
```



在 Master（192.168.65.100）节点启动 NFS
```
systemctl enable rpcbind
systemctl enable nfs-server
systemctl start rpcbind
systemctl start nfs-server
```



在 Master（192.168.65.100）节点加载配置：
exportfs -r


在 Master（192.168.65.100）节点检查配置是否生效：
exportfs



在 Node（192.168.65.101、192.168.65.102）节点安装 nfs-utils
```
# 服务器端防火墙开放111、662、875、892、2049的 tcp / udp 允许，否则远端客户无法连接。
yum install -y nfs-utils
```



在 Node（192.168.65.101、192.168.65.102）节点，执行以下命令检查 nfs 服务器端是否有设置共享目录

```
# showmount -e $(nfs服务器的IP)
showmount -e 192.168.65.100
```



在 Node（192.168.65.101、192.168.65.102）节点，执行以下命令挂载 nfs 服务器上的共享目录到本机路径 /root/nd

```
mkdir /nd

# mount -t nfs $(nfs服务器的IP):/root/nfs_root /root/nfsmount
mount -t nfs 192.168.65.100:/nfs/data /nd

```


在 Node （192.168.65.101）节点写入一个测试文件
```
echo "hello nfs server" > /nd/test.txt
```



在 Master（192.168.65.100）节点验证文件是否写入成功
cat /nfs/data/test.txt



### 2.1.2 正式挂载

vi k8s-nginx-nfs.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.20.2
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 200Mi
    ports:
    - containerPort: 80
      name:  http
    volumeMounts:
    - name: localtime
      mountPath: /etc/localtime
    - name: html  
      mountPath: /usr/share/nginx/html/ # / 一定是文件夹
  volumes:
    - name: localtime
      hostPath:
        path: /usr/share/zoneinfo/Asia/Shanghai
    - name: html 
      nfs: # 使用 nfs 存储驱动
        path: /nfs/data  # nfs 共享的目录
        server:  192.168.65.100  # nfs 服务端的 IP 地址或 hostname 
  restartPolicy: Always
```

kubectl apply -f k8s-nginx-nfs.yaml

![32.gif](https://cdn.nlark.com/yuque/0/2022/gif/513185/1648539099242-3d5459e5-4561-4eba-bd9c-4cd16b006b18.gif)



# 3 静态供应: PV 和 PVC


- 前面我们已经学习了使用 NFS 提供存储，此时就要求用户会搭建 NFS 系统，并且会在 yaml 配置 NFS，这就带来的一些问题：
    - ① 开发人员对 Pod 很熟悉，非常清楚 Pod 中的容器那些位置适合挂载出去。但是，由于 Kubernetes 支持的存储系统非常之多，开发人员并不清楚底层的存储系统，而且要求开发人员全部熟悉，不太可能（术业有专攻，运维人员比较熟悉存储系统）。
    - ② 在 yaml 中配置存储系统，就意味着将存储系统的配置信息暴露，非常不安全（容易造成泄露）。
- 为了能够屏蔽底层存储实现的细节，方便用户使用，Kubernetes 引入了 PV 和 PVC 两种资源对象。

## 3.1 概述

PV（PersistentVolume）— 提供者视角
- PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 PV 由 Kubernetes 管理员进行创建和配置，它和底层具体的共享存储技术有关，并通过插件完成和共享存储的对接。
- 是一个已经配置好的 **存储资源**，由管理员或动态存储插件（如 EBS CSI）创建。
- 是对底层存储（如 EBS、NFS、EFS）的抽象封装。
- 类似“房东提供的房子”。

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/data
```



PVC（PersistentVolumeClaim）— 使用者视角
- PVC（Persistent Volume Claim）是持久化卷声明的意思，是用户对于存储需求的一种声明。换言之，PVC 其实就是用户向Kubernetes 系统发出的一种资源需求申请。
- 是用户/Pod 向 Kubernetes 提出的一个存储请求。
- 指定想要的容量、访问模式等。
- 类似“租户想租一个符合要求的房子”。

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```


---

PV 和 pvc 之间的关系
- Kubernetes 会**自动匹配** PVC 和合适的 PV（如果是静态分配），或由存储类动态创建 PV。
- Pod 通过挂载 PVC，间接访问背后的存储卷。


---

Persistent Volume (PV)
• Survives pod restarts
• HostPath PV
• Local storage PV
• External storage systems
• PVs are attached via PV claims

PV Claims (PVC)
- Dynamic
- Abstraction to underlying storage
- ReadWriteOnce

![](image/Pasted%20image%2020240711180543.png)


----


PV 的缺点：也有人称 PV 为静态供应。
- ① 需要运维事先准备好 PV 池。
- ② 资源浪费：没有办法预估合适的 PV，假设运维向 k8s 申请了 20m 、50m、10G 的 PV，而开发人员申请 2G 的 PVC ，那么就会匹配到 10G 的PV ，这样会造成 8G 的空间浪费。


PVC的注意点：
- Pod 的删除，并不会影响 PVC；换言之，PVC 可以独立于 Pod 存在，PVC 也是 K8s 的系统资源。不过，推荐将 PVC 和 Pod 也在一个 yaml 文件中。
- PVC 删除会不会影响到 PV，要根据 PV 的回收策略决定。

## 3.2 PV 和PVC如何联合使用的




![](https://cdn.nlark.com/yuque/0/2022/png/513185/1648539110775-c090d1e7-d19d-4bdd-9776-ac1895d5956f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_26%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

  ![](image/Pasted%20image%2020240711180651.png)

![](image/Pasted%20image%2020240711180754.png)

![](image/Pasted%20image%2020240711180905.png)

![](image/Pasted%20image%2020240711181625.png)


## 3.3 PVC和PV的match

PV 和 pvc 之间的关系
- Kubernetes 会**自动匹配** PVC 和合适的 PV（如果是静态分配），或由存储类动态创建 PV。
- Pod 通过挂载 PVC，间接访问背后的存储卷。


For a PVC to successfully bind to a PV, certain attributes between the two must match.
1. **Storage Class**:
    - The `storageClassName` in both the PV and PVC must match unless they are using the default storage class (if no storage class is specified).
    - If a PVC requests a specific storage class, it will only bind to a PV with the same storage class.
2. **Access Modes**:
    - The `accessModes` specified in the PVC must be supported by the `accessModes` of the PV. Common access modes include:
        - `ReadWriteOnce` (RWO): The volume can be mounted as read-write by a single node.
        - `ReadOnlyMany` (ROX): The volume can be mounted as read-only by many nodes.
        - `ReadWriteMany` (RWX): The volume can be mounted as read-write by many nodes.
    - The PV must support at least the access mode requested by the PVC.
3. **Capacity**:
    - The storage capacity requested by the PVC (e.g., `5Gi`) must be less than or equal to the capacity offered by the PV. A PV with insufficient capacity will not bind to a PVC requesting more storage.
4. **Selector Labels** (Optional):
    - A PVC can include a `selector` to specify labels that must match on the PV. If labels are used, the PVC will only bind to PVs that match the specified labels.

Example Scenario:
PV Definition
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: standard

```

PVC Definition:
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: standard

```

In this example:

    The storageClassName matches ("standard").
    The accessModes match (ReadWriteOnce).
    The PV offers 10Gi of storage, and the PVC requests 5Gi, which is sufficient.

Thus, the PVC will bind to the PV.


可以通过 kubectl -n chenqiang-pv-test get pvc 查看匹配关系 

### 3.3.1 关联pvc到特定的pv, matchlabels

```
-[appuser@chenqiang-dev pvtest]$ cat nfs-pv2.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv2
  namespace: chenqiang-pv-test
  labels:
    pv: nfs-pv2   # 这里 
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  nfs:
    # FIXME: use the right IP
    server: 10.130.44.20
    path: "/test/mysql-nfs01"
```

```
-[appuser@chenqiang-dev pvtest]$ cat nfs-pvc2.yaml   
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc2
  namespace: chenqiang-pv-test
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 90Mi
  selector:
    matchLabels:   # 这里
      pv: nfs-pv2
```

```
-[appuser@chenqiang-dev pvtest]$ kubectl -n chenqiang-pv-test get pvc
NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-pvc1   Bound     nfs-pv1   100Mi      RWX                           41m
nfs-pvc2   Bound     nfs-pv2   100Mi      RWX                           25s
nfs-pvc3   Bound     nfs-pv3   100Mi      RWX                           17s
nfs-pvc4   Bound     nfs-pv4   100Mi      RWX                           10s
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/qianggezhishen/article/details/80764378
```
## 3.4 PVC 和 PV 的基本演示

![](image/Pasted%20image%2020240711181429.png)

### 3.4.1 创建 PV 
（一般是运维人员操作）

```
mkdir -pv /nfs/data/10m
mkdir -pv /nfs/data/20m
mkdir -pv /nfs/data/500m
mkdir -pv /nfs/data/1Gi
```


vi k8s-pv.yaml

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-10m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 10m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/10m # nfs 共享的目录，mkdir -pv /nfs/data/10m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
    
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-20m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 20m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/20m # nfs 共享的目录，mkdir -pv /nfs/data/20m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
    
--- 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-500m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 500m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/500m # nfs 共享的目录，mkdir -pv /nfs/data/500m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
    
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-1g
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/1Gi # nfs 共享的目录，mkdir -pv /nfs/data/1Gi
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
```

kubectl apply -f k8s-pv.yaml


---

### 3.4.2 创建 PVC
(一般是开发人员)

vi k8s-pvc.yaml

```

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc-500m
  namespace: default
  labels:
    app: nginx-pvc-500m
spec:
  storageClassName: nfs-storage
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500m
      
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.20.2
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 200Mi
    ports:
    - containerPort:  80
      name:  http
    volumeMounts:
    - name: localtime
      mountPath: /etc/localtime
    - name: html
      mountPath: /usr/share/nginx/html/ 
  volumes:
    - name: localtime
      hostPath:
        path: /usr/share/zoneinfo/Asia/Shanghai
    - name: html    
      persistentVolumeClaim:
        claimName:  nginx-pvc-500m
        readOnly: false  
  restartPolicy: Always

```

kubectl apply -f k8s-pvc.yaml


注意：
- pv 和 pvc 的 accessModes 和 storageClassName 必须一致。
- pvc 申请的空间大小不大于 pv 的空间大小。


- storageClassName  就相当于分组的组名，通过 storageClassName 可以区分不同类型的存储驱动，主要是为了方便管理。

![36.png](https://cdn.nlark.com/yuque/0/2022/png/513185/1648539143062-f339da24-4f86-489f-bba0-53916fae789c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_44%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

## 3.5 pod 使用 PVC 

```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  volumes:
    - name: my-storage
      persistentVolumeClaim:
        claimName: my-pvc
  containers:
    - name: my-container
      image: nginx
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: my-storage
```

## 3.6 PV 的属性 


![](image/Pasted%20image%2020240711180920.png)

LocalPersistentVolumnes
![](image/Pasted%20image%2020240711181136.png)


选择PV的卷类型 
![](image/Pasted%20image%2020240711181805.png)




## 3.7 PV的回收策略 

- 目前的回收策略有：
    - Retain：手动回收（默认）。
    - Recycle：基本擦除 (`rm -rf /thevolume/*`)。
    - Delete：诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除。
- 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。



### 3.7.1 Retain


1 创建 pv
```
mkdir -pv /nfs/data/10m
mkdir -pv /nfs/data/20m
mkdir -pv /nfs/data/500m
mkdir -pv /nfs/data/1Gi
```


vi k8s-pv.yaml

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-10m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 10m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/10m # nfs 共享的目录，mkdir -pv /nfs/data/10m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
    
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-20m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 20m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/20m # nfs 共享的目录，mkdir -pv /nfs/data/20m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
    
--- 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-500m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 500m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/500m # nfs 共享的目录，mkdir -pv /nfs/data/500m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
    
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-1g
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/1Gi # nfs 共享的目录，mkdir -pv /nfs/data/1Gi
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname


```

kubectl apply -f k8s-pv.yaml

echo 111 > /nfs/data/500m/index.html

----

2 创建 pvc

vi k8s-pvc.yaml

```

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc-500m
  namespace: default
  labels:
    app: nginx-pvc-500m
spec:
  storageClassName: nfs-storage
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500m
```


kubectl apply -f k8s-pvc.yaml
kubectl delete -f k8s-pvc.yaml





### 3.7.2 Recycle

1 创建pv

```
mkdir -pv /nfs/data/10m
mkdir -pv /nfs/data/20m
mkdir -pv /nfs/data/500m
mkdir -pv /nfs/data/1Gi
```

vi k8s-pv.yaml

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-10m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 10m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/10m # nfs 共享的目录，mkdir -pv /nfs/data/10m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
  persistentVolumeReclaimPolicy: Recycle  # 回收策略
  
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-20m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 20m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/20m # nfs 共享的目录，mkdir -pv /nfs/data/20m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
  persistentVolumeReclaimPolicy: Recycle  # 回收策略  
  
--- 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-500m
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 500m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/500m # nfs 共享的目录，mkdir -pv /nfs/data/500m
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
  persistentVolumeReclaimPolicy: Recycle  # 回收策略  
  
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-1g
spec:
  storageClassName: nfs-storage # 用于分组
  capacity:
    storage: 20m
  accessModes:
    - ReadWriteOnce
  nfs: # 使用 nfs 存储驱动
    path: /nfs/data/1Gi # nfs 共享的目录，mkdir -pv /nfs/data/1Gi
    server: 192.168.65.100 # nfs 服务端的 IP 地址或 hostname
  persistentVolumeReclaimPolicy: Recycle  # 回收策略

```

kubectl apply -f k8s-pv.yaml

echo 111 > /nfs/data/500m/index.html


2 创建 pvc

vi k8s-pvc.yaml

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc-500m
  namespace: default
  labels:
    app: nginx-pvc-500m
spec:
  storageClassName: nfs-storage
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500m
```

kubectl apply -f k8s-pvc.yaml
kubectl delete -f k8s-pvc.yaml

## 3.8 PV 的访问模式

- 访问模式（accessModes）：用来描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：
    - ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载。
    - ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载。
    - ReadWriteMany（RWX）：读写权限，可以被多个节点挂载。

## 3.9 PV 的生命周期 

- 一个 PV 的生命周期，可能会处于 4 种不同的阶段：
    - Available（可用）：表示可用状态，还未被任何 PVC 绑定。
    - Bound（已绑定）：表示 PV 已经被 PVC 绑定。
    - Released（已释放）：表示 PVC 被删除，但是资源还没有被集群重新释放。
    - Failed（失败）：表示该 PV 的自动回收失败。



![39.png](https://cdn.nlark.com/yuque/0/2022/png/513185/1648539183133-b087587e-2fba-4f90-b4c0-07d62f088488.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_35%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)


![](image/Pasted%20image%2020240711181306.png)


- PVC 和 PV 是一一对应的，PV 和 PVC 之间的相互作用遵循如下的生命周期：
- ① 资源供应：管理员手动创建底层存储和 PV。
- ② 资源绑定：
    - 用户创建 PVC ，Kubernetes 负责根据 PVC 声明去寻找 PV ，并绑定在用户定义好 PVC 之后，系统将根据 PVC 对存储资源的请求在以存在的 PV 中选择一个满足条件的。
        - 一旦找到，就将该 PV 和用户定义的 PVC 进行绑定，用户的应用就可以使用这个 PVC 了。
        - 如果找不到，PVC 就会无限期的处于 Pending 状态，直到系统管理员创建一个符合其要求的 PV 。
    - ==PV 一旦绑定到某个 PVC 上，就会被这个 PVC 独占，不能再和其他的 PVC 进行绑定了。==

- ③ 资源使用：用户可以在 Pod 中像 Volume 一样使用 PVC ，Pod 使用 Volume 的定义，将 PVC 挂载到容器内的某个路径进行使用。

- ④ 资源释放：
    - 用户删除 PVC 来释放 PV 。
    - 当存储资源使用完毕后，用户可以删除 PVC，和该 PVC 绑定的 PV 将会标记为 `已释放` ，但是还不能立刻和其他的 PVC 进行绑定。通过之前 PVC 写入的数据可能还留在存储设备上，只有在清除之后该 PV 才能再次使用。

- ⑤ 资源回收：
    - Kubernetes 根据 PV 设置的回收策略进行资源的回收。
    - 对于 PV，管理员可以设定回收策略，用于设置与之绑定的 PVC 释放资源之后如何处理遗留数据的问题。只有 PV 的存储空间完成回收，才能供新的 PVC 绑定和使用。



# 4 动态供应 

## 4.1 概述

- 静态供应：集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。
- 动态供应：集群自动根据 PVC 创建出对应 PV 进行使用。

![40.png](https://cdn.nlark.com/yuque/0/2022/png/513185/1648539190719-9b6bc9f5-abd0-435e-a518-18fab195e712.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_43%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)


## 4.2 StorageClass 

==在 Kubernetes 中，**StorageClass 是全局资源**，不属于任何命名空间。因此，你**可以在任何 namespace 中使用已存在的 StorageClass**，即使它是在 `kube-system` 中创建的。==



https://www.cnblogs.com/zuoyang/p/16409196.html

存储类（storage class）是 Kubernetes 资源类型的一种，它是由管理员为管理 PV 之便而按需创建的类别（逻辑组），例如可按存储系统的性能高低分类，或者根据其综合服务质量级别进行分类、依照备份策略分类，甚至直接按管理员自定义的标准进行分类等。

Kubernetes 自身无法理解 "类别" 到底意味着什么，它仅仅是将这些当作 PV 的特性描述。

存储类（StorageClass）的优势之一是支持 PV 的动态创建。用户用到持久性存储时，需要通过创建 PVC 来绑定匹配的 PV，此类操作需求量较大，或者当管理员手动创建的 PV 无法满足 PVC 的所有需求时，系统按 PVC 的需求标准动态创建适配的 PV 会为存储管理带来极大的灵活性。

存储类对象的名称至关重要，它是用户调用的标识。创建存储类对象时，除了名称之外，还需要为其定义三个关键字段：

```
[root@mh-k8s-master-247-10 ~]# kubectl explain storageclass
KIND:     StorageClass
VERSION:  storage.k8s.io/v1
 
DESCRIPTION:
     StorageClass describes the parameters for a class of storage for which
     PersistentVolumes can be dynamically provisioned. StorageClasses are
     non-namespaced; the name of the storage class according to etcd is in
     ObjectMeta.Name.
 
FIELDS:
   allowVolumeExpansion <boolean>
     AllowVolumeExpansion shows whether the storage class allow volume expand
 
   allowedTopologies    <[]Object>
     Restrict the node topologies where volumes can be dynamically provisioned.
     Each volume plugin defines its own supported topology specifications. An
     empty TopologySelectorTerm list means there is no topology restriction.
     This field is only honored by servers that enable the VolumeScheduling
     feature.
 
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
 
   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
 
   metadata <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
 
   mountOptions <[]string>
     Dynamically provisioned PersistentVolumes of this storage class are created
     with these mountOptions, e.g. ["ro", "soft"]. Not validated - mount of the
     PVs will simply fail if one is invalid.
 
   parameters   <map[string]string>
     Parameters holds the parameters for the provisioner that should create
     volumes of this storage class.
 
   provisioner  <string> -required-
     Provisioner indicates the type of the provisioner.
 
   reclaimPolicy    <string>
     Dynamically provisioned PersistentVolumes of this storage class are created
     with this reclaimPolicy. Defaults to Delete.
 
   volumeBindingMode    <string>
     VolumeBindingMode indicates how PersistentVolumeClaims should be
     provisioned and bound. When unset, VolumeBindingImmediate is used. This
     field is only honored by servers that enable the VolumeScheduling feature.
 
[root@mh-k8s-master-247-10 ~]#
```


- `reclaimPolicy <string>`: 为当前存储类动态创建的 PV 指定回收策略，可用值为 Delete（默认）和 Retain；不过，那些由管理员手工创建的 PV 的回收策略取决于它们自身的定义。
- `volumeBindingMode <string>`: 定义如何为 PVC 完成供给和绑定，默认值为 "VolumeBindingImmediate"；此选项仅在启用了存储卷调度功能时才能生效。
- `mountOptions <[]string>`：由当前类动态创建的 PV 的挂载选项列表。使用这些安装选项，例如`[“ro”，“soft”]`。未验证的， 挂载 PV卷 将会无效。


---

定义一个使用 Gluster 存储系统的存储类 glusterfs

  下面是一个定义在 glusterfs-storageclass.yaml 配置文件的资源清单，它定义了一个使用 Gluster 存储系统的存储类 glusterfs，并通过 annotations 字段将其定义为默认的存储类：
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gluster-vol-default
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://192.168.10.100:8080"
  restuser: ""
  secretNamespace: ""
  secretName: ""
allowVolumeExpansion: true
```


----

通过定义PVC实现PV 的动态供给/动态创建

动态 PV 供给的启用，需要事先由管理员创建至少一个存储类，不同的 Provisoner 的创建方法各有不同，另外，并非所有的存储卷插件都由 Kubernetes 内建支持 PV 动态供给功能。

下面的资源清单定义在 pvc-glusterfs-dynamic-0001.yaml 配置文件中，它将从 glusterfs 存储类中申请适应 5GB 的存储空间。

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-gluser-dynamic-0001
  annotations:
    volume.beta.kubernetes.io/storage-class: glusterfs
spec:
  # storageClassName: "glusterfs"
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

目前，在 PVC 的定义中指定使用的存储类资源的方式共有两种：一种是使用 spec.storageClassName 字段，另一种是使用` "volume.beta.kubernetes.io/storage-class" `注解信息。不过，建议仅使用一种方式，以免两者设置为不同的值时会出现配置错误。接下来创建定义的 PVC，并检查其绑定状态：

通过如下命令输出的 PVC 资源的描述信息可以看到 PVC 存储卷创建和绑定情况，绑定的 PV 资源由 persistentvolume-controller 控制器动态提供。
kubectl  describe pvc pvc-gluster-dynamic-0001

任何支持 PV 动态供给的存储系统都可以在定义为存储类后由 PVC 动态申请使用，这对于难以事先预估使用到的存储空间大小及存储卷数量的使用场景尤为有用，例如由 StatefuleSet 控制器管理 Pod 对象时，存储卷是必备资源，且随着规模的变动，存储卷的数量也会随着变动。


### 4.2.1 container 没有 schreibzugriff auf dem volume : spec.securityContext.fsGroup: 1001


pod 中 会创建一个 container  和一个 volumne,   volumen 会挂在到container上 
- 容器内默认用户所属的组 ID 可能是 1001.
- volume 通过 pvc 创造 , pvc 中指定了 storageclass 

问题是 container 没有 schreibzugriff auf dem volume 
如何解决问题  
- storageclass 的 spec 中 就制定好一些配置
- 见下面

---

die storageClass sollte (sofern sie POSIX file permissions bis zum container durchreicht, z.B. EFS, azure-disk) die passenden permissions (schreibbar für gid 1001) haben. falls das nicht der fall ist, kann `podSecurityContext.fsGroup: <gid>` um die group id ergänzt werden, die schreibzugriff auf dem volume hat

Wenn die storageClass   die POSIX file permissions bis zum container durchreichen kann, sollte  die storageClass  passenden permissions schon haben. die permissions erlaubt 

如果 StorageClass（例如 EFS、azure-disk）能够将 POSIX 文件权限传递给容器，那么它应该具备合适的权限（即对 GID 为 1001 的组可写）。
如果不是这种情况，则可以通过 `podSecurityContext.fsGroup: <gid>` 添加具有对该卷写权限的 组 ID。


---

- POSIX **POSIX** 是 “**Portable Operating System Interface**” 的缩写，意思是“可移植操作系统接口”。当我们在存储卷（如 NFS、EFS、Azure Disk）上谈到“**POSIX 权限**”时，指的是该文件系统是否支持 Unix/Linux 传统的**用户、组和权限模型**。
- 某些 StorageClass（如 **EFS** 或 **Azure Disk**）支持传统的 **Linux 文件权限（POSIX）**。
- 你希望容器中的某个进程能够对挂载的卷写入，这通常依赖于它的**组 ID（GID）**是否有写权限 对挂载的卷。
- 如果默认情况下容器无法写入挂载卷，可以设置：, 这会让整个 Pod 的挂载卷的**文件组权限**使用 GID `1001`，从而允许对应的进程写入。
```
# in a pod manifest 

spec:
    securityContext:
      fsGroup: 1001
```
- 这是在 **Linux 内核挂载文件系统**时通过 chgrp 和 chmod 模拟出来的行为。
- 它主要作用于 **PersistentVolume** 类型的卷（如 EFS、Azure Disk、NFS）。
- 它不影响容器中进程的主 GID，只是让挂载目录对 GID 为 1001 的进程变得可访问。

- GID 1001 是指 **Linux 系统中的一个组 ID（Group ID）为 1001 的用户组**。
    - 在许多镜像（如某些官方 Node.js、PostgreSQL、Nginx、Python 等）中，==容器内默认用户所属的组 ID 可能是 1001.==
- 如果卷（Volume）挂载进来时，其权限没有包含 GID 1001 的写权限，容器内的应用就无法写入。
- 解决办法：**要么给卷设置相应的 POSIX 权限，要么通过 `fsGroup: 1001` 来强制赋予写权限。**
    - ==Kubernetes Pod 加上 `fsGroup: 1001`: Kubernetes 会尝试把 **所有支持 POSIX 权限的卷** 挂载进容器时的目录权限修改 成 **其属组为 GID 1001**，并赋予该组 **读/写/执行权限（g+rwx）**==。
    - Kubernetes versucht, bei allen POSIX-kompatiblen Volumes den **Gruppenbesitzer auf GID 1001** zu setzen und der Gruppe **Lese-, Schreib- und Ausführungsrechte (g+rwx)** zu gewähren, wenn sie (Volumes) ins Container-Dateisystem eingehängt werden
    - Kubernetes will attempt to set the **group ownership to GID 1001** and grant the group **read, write, and execute permissions (g+rwx)** for all POSIX-compliant volumes when they are mounted into the container filesystem.


这会确保 `/data` 路径的文件属于 GID 1001 的组，并具有相应的写权限（`g+w`）。
```apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  securityContext:
    fsGroup: 1001
  containers:
  - name: app
    image: your-image
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: your-pvc
```

---

spec.securityContext.fsGroup: 1001 这样的的  会触发的行为

假设你有一个挂载卷 /data，原始权限为：
`drwxr-xr-x root root /data`

如果 Pod 指定了：
```
securityContext:
  fsGroup: 1001
```

挂载之后，Kubernetes 会尝试变成：
`drwxrwsr-x root 1001 /data`

这样容器中属于 GID 1001 的进程就可以写入 `/data`。


### 4.2.2 Terraform 中定义StorageClass:  die zugehörigen PV, Daten sowie der Access Point mitgelöscht wird, wenn pvc gelöscht wird 

ivu: efs mount options: Daten löschen, wenn PVC gelöscht
https://confluence.ivu.de/jira/browse/DEVOPS-3731

efs mount options so konfigurieren, dass Daten gelöscht werden, wenn PVC gelöscht wird, für alle efs classes
- [https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/411#issuecomment-819007483](https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/411#issuecomment-819007483 "Verknüpfung folgen")
    - configure the storageclass with reclaimPolicy: Delete  and set --delete-access-point-root-dir=true in depolyment efs-csi-controller in kube-system namespace
- [https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/1476](https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/1476#issuecomment-2459971941 "Verknüpfung folgen")
    - upload the aws-efs-csi-driver  into latest versions “v2.1.7-eksbuild.1” in order to fix the problem which can be solved in alternative way by adding manuelly AWS_DEFAULT_REGION into the depolyment efs-csi-controller in kube-system namespace
- [https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/1382#issuecomment-2195235714](https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/1382#issuecomment-2195235714 "Verknüpfung folgen")
    - set hostNetwork: true and  in Depolymen efs-csi-container  in kube-system namespace
- [https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/832#issuecomment-1545944146](https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/832#issuecomment-1545944146 "Verknüpfung folgen") and [https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/7e6e064fc62bc007922e1f74bfb72a4d18209b3c/examples/kubernetes/dynamic_provisioning/README.md?plain=1#L159](https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/7e6e064fc62bc007922e1f74bfb72a4d18209b3c/examples/kubernetes/dynamic_provisioning/README.md?plain=1#L159 "Verknüpfung folgen") 
    - The target Elastic File System's policy must allow these actions: "elasticfilesystem:ClientRootAccess", "elasticfilesystem:ClientWrite", "elasticfilesystem:ClientMount"

Test:
-   Using Network File System to mount EFS file systems on linux ec2 instance (worker node)
    - [https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-old.html](https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-old.html "Verknüpfung folgen")
- Dynamic Provisioning
    - [https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/examples/kubernetes/dynamic_provisioning](https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/examples/kubernetes/dynamic_provisioning "Verknüpfung folgen")


```yaml

#### _______________________________________________________________ Lookup AWS resources _____ ####

locals {
  resource_prefix                           = "eks-${var.cluster_name}"
  efs_file_system_tags                      = var.efs_file_system_id == null? { Name = local.resource_prefix } : null
  vpc_tags                                  = var.vpc_id == null? { Name = local.resource_prefix } : null
  aws_load_balancer_controller_iam_role_arn = (
    var.aws_load_balancer_controller_iam_role_arn == null? 
    data.aws_iam_role.aws_load_balancer_controller[0].arn : 
    var.aws_load_balancer_controller_iam_role_arn
  )
}

data "aws_region" "eks" {}

data "aws_efs_file_system" "eks" {
  file_system_id = var.efs_file_system_id
  tags           = local.efs_file_system_tags
}


#### _______________________________________________________________ Kubernetes resources _____ ####
locals {
  # Manifest for efs storage class. See also: 
  # https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md
  # https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml
  efs_storage_class_manifest = <<-EOF
    kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: efs-sc
      annotations:
        storageclass.kubernetes.io/is-default-class: "true"
    provisioner: efs.csi.aws.com
    reclaimPolicy: Delete     ## 注意这里 
    parameters:
      provisioningMode: efs-ap
      fileSystemId: ${data.aws_efs_file_system.eks.id}
      directoryPerms: "700" # without specifying it, there is an error "failed to provision volume with StorageClass ... minimum field size of 3 ..."
      basePath: "/" # to avoid too long EFS path names
      subPathPattern: "$${.PVC.namespace}/$${.PVC.name}"
      ensureUniqueDirectory: "false" # to avoid too long EFS path names
      # gidRangeStart: "1000" # optional
      # gidRangeEnd: "2000" # optional
      # reuseAccessPoint: "false" # optional
  EOF


  # Manifest for efs storage class for postgres containers. More info:
  # https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/300
  postgres_efs_storage_class_manifest = <<-EOF
    kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: postgr-efs
    provisioner: efs.csi.aws.com
    parameters:
      provisioningMode: efs-ap
      fileSystemId: ${data.aws_efs_file_system.eks.id}
      directoryPerms: "700" # without specifying it, there is an error "failed to provision volume with StorageClass ... minimum field size of 3 ..."
      basePath: "/" # to avoid too long EFS path names
      subPathPattern: "$${.PVC.namespace}/$${.PVC.name}"
      ensureUniqueDirectory: "false" # to avoid too long EFS path names
      gid: "1001"
      uid: "1001"
  EOF


  # Patch the Deployment efs-csi-controller which serves for efs-csi-driver in eks cluster. 
  # The original deployment efs-csi-controller was already created along with provisioning the resource "aws_eks_addon" with name "aws_efs_csi_driver" in modules\cluster\main.tf. See the original spec of Deployment efs-csi-controller https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/deploy/kubernetes/base/controller-deployment.yaml#L40-L41
  # This json code is used to patch the Deployment resource with the following purposes:
  # add the --delete-access-point-root-dir=true argument to the efs-csi-controller Deployment. This is needed to delete the contents of the EFS Access Point when deleting a Persistent Volume (PV) with reclaimPolicy: Delete. See also https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/411#issuecomment-819205846
  # Annotation "kubectl.kubernetes.io/restartedAt" is used to force a redeployment when the timestamp is updated. Terraform dynamically replaces the timestamp during the terraform apply.
  # the efs-csi-controller tried to mount the EFS on worker node, but it couldn't reach the EFS because the Security Group only allowed traffic with the CIDR of my worker nodes. The network of the Pod (on which the deployment efs-csi-controller runs) is different from that of the worker node, so the controller couldn't mount the EFS or delete the access point and its content, causing it to get stuck in the process. To solve this, I added the label hostNetwork: true on the spec of deployment efs-csi-controller, and it worked. See also: https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/1382#issuecomment-2195235714 
  efs_csi_controller_restart_patch_json = jsonencode({
    spec = {
      template = {
        metadata = {
          annotations = {
            "kubectl.kubernetes.io/restartedAt" = timestamp()
          }
        }
        spec = {
          containers = [
            {
              name = "efs-plugin"
              args = [
                "--endpoint=$(CSI_ENDPOINT)",
                "--logtostderr",
                "--v=2",
                "--delete-access-point-root-dir=true"   # 注意这里
              ]
            }
          ] 
          hostNetwork = true  # 注意这里
        }
      }
    }
  })

}


resource "kubernetes_manifest" "efs_storage_class" {
  manifest = yamldecode(local.efs_storage_class_manifest)
}

resource "kubernetes_manifest" "postgres_efs_storage_class" {
  manifest = yamldecode(local.postgres_efs_storage_class_manifest)
}


# Terraform resource kubernetes_manifest can not be used here because the kubernetes_manifest resource does not support patching. It is only used for creating new resources. It will returns a error that the kubernetes resource, which should be generated by executing  Terraform resource kubernetes_manifest, already exists.
# resource null_resource is to execute kubectl patch command.It can also work with patching functionality. 
resource "null_resource" "efs_csi_controller_deployment_patch" {
  provisioner "local-exec" {
    command = "kubectl patch deployment efs-csi-controller -n kube-system --type=strategic --patch=${local.efs_csi_controller_restart_patch_json}"
  }
  triggers = {
    content_hash = sha1(local.efs_csi_controller_restart_patch_json) # to force the local-exec provisioner to run again if the content of the patch changes. Otherwise, it will not run again once this null_resource is already created.
  }
}
```


aws-efc-csi-driver 的 iam_policies 

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:DescribeAccessPoints",
        "elasticfilesystem:DescribeFileSystems",
        "elasticfilesystem:DescribeMountTargets",
        "ec2:DescribeAvailabilityZones"
      ],
      "Resource": "*"
    },
    {  # 注意这里 使得 aws-efc-csi-driver   可以 对 elasticfile 中的daten 做一些操作 
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:ClientWrite",
        "elasticfilesystem:ClientRootAccess",
        "elasticfilesystem:ClientMount"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:CreateAccessPoint"
      ],
      "Resource": "*",
      "Condition": {
        "StringLike": {
          "aws:RequestTag/efs.csi.aws.com/cluster": "true"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:TagResource"
      ],
      "Resource": "*",
      "Condition": {
        "StringLike": {
          "aws:ResourceTag/efs.csi.aws.com/cluster": "true"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": "elasticfilesystem:DeleteAccessPoint",
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/efs.csi.aws.com/cluster": "true"
        }
      }
    }
  ]
}
```



## 4.3 动态供应的完整流程


![](image/Pasted%20image%2020240613134008.png)


● ① 集群管理员预先创建存储类（StorageClass）。
● ② 用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)。
● ③ 存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)。
● ④ 系统读取存储类的信息。
● ⑤ 系统基于存储类的信息，在后台自动创建 PVC 需要的 PV 。
● ⑥ 用户创建一个使用 PVC 的 Pod 。
● ⑦ Pod 中的应用通过 PVC 进行数据的持久化。
● ⑧ PVC 使用 PV 进行数据的最终持久化处理


## 4.4 设置 NFS 动态供应

注意：不一定需要设置 NFS 动态供应，可以直接使用云厂商提供的 StorageClass 。

- [官网地址](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)。


### 4.4.1 部署 NFS 动态供应：

vi k8s-nfs-provisioner.yaml

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # 指定一个供应商的名字 
# or choose another name, 必须匹配 deployment 的 env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false" # 删除 PV 的时候，PV 中的内容是否备份
  
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: ccr.ccs.tencentyun.com/gcr-containers/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.65.100 # NFS 服务器的地址
            - name: NFS_PATH
              value: /nfs/data # NFS 服务器的共享目录
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.65.100
            path: /nfs/data
            
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  namespace: default
  
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
    
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
  
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
    
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```


kubectl apply -f k8s-nfs-provisioner.yaml


### 4.4.2 测试动态供应

vi k8s-pvc.yaml


```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
  namespace: default
  labels:
    app: nginx-pvc
spec:
  storageClassName: nfs-client # 注意此处
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.20.2
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 200Mi
    ports:
    - containerPort:  80
      name:  http
    volumeMounts:
    - name: localtime
      mountPath: /etc/localtime
    - name: html
      mountPath: /usr/share/nginx/html/ 
  volumes:
    - name: localtime
      hostPath:
        path: /usr/share/zoneinfo/Asia/Shanghai
    - name: html    
      persistentVolumeClaim:
        claimName:  nginx-pvc
        readOnly: false  
  restartPolicy: Always
```

kubectl apply -f k8s-pvc.yaml



## 4.5 设置 SC 为默认驱动


```
命令
kubectl patch storageclass <your-class-name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

设置 SC 为默认驱动
kubectl patch storageclass nfs-client -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

```

## 4.6 测试默认驱动

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
  namespace: default
  labels:
    app: nginx-pvc
spec:
  # storageClassName: nfs-client 不写，就使用默认的
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
      
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.20.2
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 200Mi
    ports:
    - containerPort:  80
      name:  http
    volumeMounts:
    - name: localtime
      mountPath: /etc/localtime
    - name: html
      mountPath: /usr/share/nginx/html/ 
  volumes:
    - name: localtime
      hostPath:
        path: /usr/share/zoneinfo/Asia/Shanghai
    - name: html    
      persistentVolumeClaim:
        claimName:  nginx-pvc
        readOnly: false  
  restartPolicy: Always
```


# 5 展望


- 目前，只需要运维人员部署好各种 storageclass，开发人员在使用的时候，创建 PVC 即可；但是，存储系统太多太多，运维人员也未必会一一掌握，此时就需要 Rook 来统一管理了。

![](image/44.webp)