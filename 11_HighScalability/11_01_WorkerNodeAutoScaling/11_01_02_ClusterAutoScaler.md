

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md


On AWS, Cluster Autoscaler utilizes Amazon EC2 Auto Scaling Groups to manage node
groups. Cluster Autoscaler typically runs as a `Deployment` in your cluster.

# 1 Requirements

Cluster Autoscaler requires Kubernetes v1.3.0 or greater.

# 2 Permissions

Cluster Autoscaler requires the ability to examine and modify EC2 Auto Scaling Groups. We recommend using [IAM roles for Service
Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) to associate the Service Account that the Cluster Autoscaler Deployment runs as
with an IAM role that is able to perform these functions. If you are unable to use IAM Roles for Service Accounts, you may associate an IAM service role with the EC2 instance on which the Cluster Autoscaler pod runs.

## 2.1 IAM Policy

There are a number of ways to run the autoscaler in AWS, which can significantly impact the range of IAM permissions required for the Cluster Autoscaler to function properly. Two options are provided below, one which will allow use of all of the features of the Cluster Autoscaler, the second with a more limited range of IAM actions enabled, which enforces using certain configuration options in the Cluster Autoscaler binary.

It is strongly recommended to restrict the target resources for the autoscaling actions by either [specifying Auto Scaling Group ARNs](https://docs.aws.amazon.com/autoscaling/latest/userguide/control-access-using-iam.html#policy-auto-scaling-resources) in the `Resource` list of the policy or
[using tag based conditionals](https://docs.aws.amazon.com/autoscaling/ec2/userguide/control-access-using-iam.html#security_iam_service-with-iam-tags). The [minimal policy](#minimal-iam-permissions-policy) includes an example of restricting by ASG ARN.

### 2.1.1 Full Cluster Autoscaler Features Policy (Recommended)

Permissions required when using [ASG Autodiscovery](#Auto-discovery-setup) and Dynamic EC2 List Generation (the default behaviour). In this example, only the second block of actions should be updated to restrict the resources/add conditionals:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeScalingActivities",
        "ec2:DescribeImages",
        "ec2:DescribeInstanceTypes",
        "ec2:DescribeLaunchTemplateVersions",
        "ec2:GetInstanceTypesFromInstanceRequirements",
        "eks:DescribeNodegroup"
      ],
      "Resource": ["*"]
    },
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup"
      ],
      "Resource": ["*"]
    }
  ]
}
```

### 2.1.2 Minimal IAM Permissions Policy

*NOTE:* The below policies/arguments to the Cluster Autoscaler need to be modified as appropriate for the names of your ASGs, as well as account ID and AWS region before being used.

The following policy provides the minimum privileges necessary for Cluster Autoscaler to run. When using this policy,==you cannot use autodiscovery of ASGs== . In addition, it restricts the IAM permissions to the node groups the Cluster Autoscaler is configured to scale.

This in turn means that you must pass the following arguments to the Cluster Autoscaler binary, replacing min and max node counts and the ASG:

```bash
--aws-use-static-instance-list=false
--nodes=1:100:exampleASG1
--nodes=1:100:exampleASG2
```

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeScalingActivities",
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup",
        "eks:DescribeNodegroup"
      ],
      "Resource": ["arn:aws:autoscaling:${YOUR_CLUSTER_AWS_REGION}:${YOUR_AWS_ACCOUNT_ID}:autoScalingGroup:*:autoScalingGroupName/${YOUR_ASG_NAME}"]
    }
  ]
}
```

The `"eks:DescribeNodegroup"` permission allows Cluster Autoscaler to pull labels and taints from the EKS DescribeNodegroup API for EKS managed nodegroups. (Note: When an EKS DescribeNodegroup API label and a tag on the underlying autoscaling group have the same key, the EKS DescribeNodegroup API label value will be saved by the Cluster Autoscaler over the autoscaling group tag value.) Currently the Cluster Autoscaler will only call the EKS DescribeNodegroup API when a managed nodegroup is created with 0 nodes and has never had any nodes added to it. Once nodes are added, even if the managed nodegroup is scaled back to 0 nodes, this functionality will not be called anymore. In the case of a Cluster Autoscaler restart, the Cluster Autoscaler will need to repopulate caches so it will call this functionality again if the managed nodegroup is at 0 nodes. Enabling this functionality any time there are 0 nodes in a managed nodegroup (even after a scale-up then scale-down) would require changes to the general shared Cluster Autoscaler code which could happen in the future.

NOTE: For private clusters, in order for the EKS DescribeNodegroup API to work, you need to create an interface endpoint for Amazon EKS (AWS PrivateLink), as described at the [AWS Documentation](https://docs.aws.amazon.com/eks/latest/userguide/vpc-interface-endpoints.html).

## 2.2 Using OIDC Federated Authentication (IAM roles for service accounts (IRSA))

OIDC federated authentication allows your service to assume an IAM role and interact with AWS services without having to store credentials as environment variables. For an example of how to use AWS IAM OIDC with the Cluster Autoscaler please see [here](CA_with_AWS_IAM_OIDC.md).

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/CA_with_AWS_IAM_OIDC.md

A) Create an IAM OIDC identity provider for your cluster with the AWS Management Console using the [documentation](https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html) .
B) Create a test [IAM policy](https://docs.aws.amazon.com/eks/latest/userguide/create-service-account-iam-policy-and-role.html) for your service accounts.

C) Create an IAM role for your service accounts in the console.
- Retrieve the OIDC issuer URL from the Amazon EKS console description of your cluster . It will look something identical to: '[https://oidc.eks.us-east-1.amazonaws.com/id/xxxxxxxxxx](https://oidc.eks.us-east-1.amazonaws.com/id/xxxxxxxxxx)'
- While creating a new IAM role, In the "Select type of trusted entity" section, choose "Web identity".
- In the "Choose a web identity provider" section: For Identity provider, choose the URL for your cluster. For Audience, type sts.amazonaws.com.
- In the "Attach Policy" section, select the policy to use for your service account, that you created in Section B above.
- After the role is created, choose the role in the console to open it for editing.
- Choose the "Trust relationships" tab, and then choose "Edit trust relationship". Edit the OIDC provider suffix and change it from :aud to :sub. Replace sts.amazonaws.com to your service account ID.
- Update trust policy to finish.

D) Set up [Cluster Autoscaler Auto-Discovery](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml) using the [tutorial](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup) .
- Open the Amazon EC2 console, and then choose EKS worker node Auto Scaling Groups from the navigation pane.
- In the "Add/Edit Auto Scaling Group Tags" window, please make sure you enter the following tags by replacing 'awsExampleClusterName' with the name of your EKS cluster. Then, choose "Save".

|Plugin|README|
|---|---|
|Key:|k8s.io/cluster-autoscaler/enabled|
|Key:|k8s.io/cluster-autoscaler/'awsExampleClusterName'|
Note: The keys for the tags that you entered don't have values. Cluster Autoscaler ignores any value set for the keys.




## 2.3 Using AWS Credentials  (not recommended)

**NOTE** The following is not recommended for Kubernetes clusters running on AWS. If you are using Amazon EKS, consider using [IAM roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) instead.

For on-premise clusters, you may create an IAM user subject to the above policy and provide the IAM credentials as environment variables in the Cluster Autoscaler deployment manifest. Cluster Autoscaler will use these credentials to
authenticate and authorize itself.

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: aws-secret
type: Opaque
data:
  aws_access_key_id: BASE64_OF_YOUR_AWS_ACCESS_KEY_ID
  aws_secret_access_key: BASE64_OF_YOUR_AWS_SECRET_ACCESS_KEY
```

Please refer to the [relevant Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) for creating a secret manually.

```yaml
env:
  - name: AWS_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        name: aws-secret
        key: aws_access_key_id
  - name: AWS_SECRET_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        name: aws-secret
        key: aws_secret_access_key
  - name: AWS_REGION
    value: YOUR_AWS_REGION
```

# 3 Auto-Discovery Setup

## 3.1 install autodiscover

Auto-Discovery Setup is the preferred method to configure Cluster Autoscaler.

Example deployment:

```
kubectl apply -f examples/cluster-autoscaler-autodiscover.yaml
```

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities", "volumeattachments"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.1
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<YOUR CLUSTER NAME>
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt # /etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
```



## 3.2 Instance types selection

Cluster Autoscaler will respect the minimum and maximum values of each Auto Scaling Group. It will only adjust the desired value.


1 <u>Instance types selection</u>
Each Auto Scaling Group should be composed of instance types that provide approximately equal capacity. For example, ASG "xlarge" could be composed of m5a.xlarge, m4.xlarge, m5.xlarge, and m5d.xlarge instance types, because each of those provide 4 vCPUs and 16GiB RAM. Separately, ASG "2xlarge" could be composed of m5a.2xlarge, m4.2xlarge, m5.2xlarge, and m5d.2xlarge instance types, because each of those provide 8 vCPUs and 32GiB RAM.
Cluster Autoscaler will attempt to determine the CPU, memory, and GPU resources provided by an Auto Scaling Group ==based on the instance type specified in its Launch Configuration or Launch Template==. It will also examine any overrides
provided in an ASG's ==Mixed Instances Policy==. If any such overrides are found, only the first instance type found will be used. See [Using Mixed Instances Policies and Spot Instances](#Using-Mixed-Instances-Policies-and-Spot-Instances) for details.


## 3.3 å¦‚ä½•è®©CA å‘ç°æŸäº›çš„ASG 

To enable this, provide the `--node-group-auto-discovery` flag as an argument whose value is a list of tag keys that should be looked for. 
For example, `--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<cluster-name>`
will find the ASGs that have at least all the given tags. Without the tags, the Cluster Autoscaler will be unable to add new instances to the ASG as it has not been discovered. In the example, a value is not given for the tags and in this case any value will be ignored and will be arbitrary - only the tag name matters. 
è¿™ä¸ªè®¾ç½®å°†ä¼šå¯»æ‰¾è‡³å°‘æ‹¥æœ‰æ‰€æœ‰è¿™äº›æŒ‡å®šæ ‡ç­¾çš„ EC2 Auto Scaling Groups (ASG)ã€‚**å¦‚æœæ²¡æœ‰è¿™äº›æ ‡ç­¾ï¼ŒCluster Autoscaler å°†æ— æ³•å‘ç°è¿™äº› ASGï¼Œå› æ­¤ä¹Ÿæ— æ³•å¯¹å…¶è¿›è¡Œæ‰©å®¹**ã€‚
åœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œ**æ ‡ç­¾æ²¡æœ‰æŒ‡å®šå…·ä½“çš„å€¼**ï¼Œæ­¤æ—¶åªè¦æ ‡ç­¾é”®å­˜åœ¨ï¼Œæ ‡ç­¾å€¼å°†è¢«å¿½ç•¥ â€”â€” ä¹Ÿå°±æ˜¯è¯´ï¼Œåªè¦æ ‡ç­¾ååŒ¹é…ï¼Œå°±èƒ½è¢«è¯†åˆ«ï¼Œ**æ ‡ç­¾å€¼å¯ä»¥æ˜¯ä»»æ„çš„**ã€‚


Optionally, the tag value can be set to be usable and custom tags can also be added.  For example,
`--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled=foo,k8s.io/cluster-autoscaler/<cluster-name>=bar,my-custom-tag=custom-value`.
Now the ASG tags must have the correct values as well as the custom tag to be successfully discovered by the Cluster Autoscaler.

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒCluster Autoscaler ä»…ä¼šå‘ç°é‚£äº›åŒæ—¶æ‹¥æœ‰ä»¥ä¸‹ä¸‰ä¸ªæ ‡ç­¾ _åŠå…¶æŒ‡å®šå€¼_ çš„ ASGï¼š
1. `k8s.io/cluster-autoscaler/enabled=foo`
2. `k8s.io/cluster-autoscaler/<cluster-name>=bar`
3. `my-custom-tag=custom-value`
åªæœ‰å½“ ASG æ‹¥æœ‰è¿™äº›å®Œæ•´çš„æ ‡ç­¾é”®å€¼å¯¹æ—¶ï¼Œæ‰èƒ½è¢«æˆåŠŸè¯†åˆ«å¹¶ç”¨äºè‡ªåŠ¨æ‰©ç¼©å®¹

---

ç»™ Cluster Autoscaler çš„ Deployment æ·»åŠ æ ‡æ³¨ï¼Œä»¥ä¾¿å®ƒè¯†åˆ«è‡ªå·±ç®¡ç†çš„é›†ç¾¤å’Œ ASGï¼š
```yaml
spec:
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"


containers:
  - name: cluster-autoscaler
    image: k8s.gcr.io/autoscaling/cluster-autoscaler:<version>
    command:
      - ./cluster-autoscaler
      - --cloud-provider=aws
      - --namespace=kube-system
      - --cluster-name=<your-cluster-name>
      - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<your-cluster-name>
      - --balance-similar-node-groups
      - --skip-nodes-with-system-pods=false


```

## 3.4 Pod Scheduling
When scaling up from 0 nodes, the Cluster Autoscaler reads ASG tags to derive information about the specifications of the nodes i.e labels and taints in that ASG. Note that it does not actually apply these labels or taints - this is done by an AWS generated user data script. ==(The ASG tags only allow the Cluster Autoscaler to discover the Auto Scaling Groups and perform specific operations on the underlying nodes. However, these ASG tags do **not** automatically label or taint the nodes with the same values as the ASG tags.)==
It gives the Cluster Autoscaler information about whether pending pods will be able to be scheduled should a new node be spun up for a particular ASG with the assumption the ASG tags accurately reflect the labels/taint actually applied.
å½“ä» 0 ä¸ªèŠ‚ç‚¹å¼€å§‹æ‰©å®¹æ—¶ï¼ŒCluster Autoscaler ä¼šè¯»å– Auto Scaling Groupï¼ˆASGï¼‰ çš„æ ‡ç­¾ï¼Œä»¥è·å–è¯¥ ASG ä¸­èŠ‚ç‚¹çš„é…ç½®ä¿¡æ¯ï¼Œä¾‹å¦‚ labelsï¼ˆæ ‡ç­¾ï¼‰ å’Œ taintsï¼ˆæ±¡ç‚¹ï¼‰ã€‚
éœ€è¦æ³¨æ„çš„æ˜¯ï¼šCluster Autoscaler æœ¬èº«å¹¶ä¸ä¼šçœŸæ­£åº”ç”¨è¿™äº›æ ‡ç­¾æˆ–æ±¡ç‚¹ â€”â€” å®é™…åº”ç”¨è¿™äº›é…ç½®çš„æ˜¯ç”± AWS ç”Ÿæˆçš„ user data è„šæœ¬ã€‚
è¿™äº› ASG æ ‡ç­¾åªæ˜¯è®© Cluster Autoscaler èƒ½å¤Ÿåˆ¤æ–­ï¼šå¦‚æœä¸ºæŸä¸ªç‰¹å®š ASG å¯åŠ¨ä¸€ä¸ªæ–°èŠ‚ç‚¹ï¼Œå½“å‰å¤„äº Pending çŠ¶æ€çš„ Pod æ˜¯å¦èƒ½å¤ŸæˆåŠŸè¢«è°ƒåº¦åˆ°è¿™ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå‰ææ˜¯å‡è®¾è¿™äº›æ ‡ç­¾å’Œæ±¡ç‚¹åœ¨èŠ‚ç‚¹å®é™…åˆ›å»ºæ—¶ä¼šè¢«å‡†ç¡®åœ°åº”ç”¨ã€‚

2.1 NodeSelector é…å¥—çš„ æ‰“åœ¨node ä¸Šçš„label 
ä»¥ä¸‹å†…å®¹ä»…åœ¨ ä» 0 ä¸ªèŠ‚ç‚¹å¼€å§‹æ‰©å®¹ æ—¶æ‰æ˜¯å¿…éœ€çš„ï¼š å¦‚æœæŸä¸ª Deployment ä½¿ç”¨äº† NodeSelectorï¼Œé‚£ä¹ˆå¯¹åº”çš„ è‡ªåŠ¨ä¼¸ç¼©ç»„ï¼ˆASGï¼‰ å¿…é¡»å¸¦æœ‰ç‰¹å®šçš„ æ ‡ç­¾ï¼ˆTagï¼‰ï¼Œå¦åˆ™ Cluster Autoscaler æ— æ³•è¯†åˆ«è¯¥ ASG æ‹¥æœ‰è¯¥æ ‡ç­¾ï¼Œä»è€Œä¸ä¼šè§¦å‘æ‰©å®¹æ“ä½œã€‚
The following is only required if scaling up from 0 nodes. The Cluster Autoscaler will require the label tag on the ASG should a deployment have a NodeSelector, else no scaling will occur as the Cluster Autoscaler does not realise the ASG has that particular label. 

The tag is of the format
`k8s.io/cluster-autoscaler/node-template/label/<label-name>`: `<label-value>` is the name of the label and the value of each tag specifies the label value.
Example tags:
- `k8s.io/cluster-autoscaler/node-template/label/foo`: `bar`

Cluster Autoscaler ä¼šé€šè¿‡ ASG ä¸Šçš„è¿™äº›æ ‡ç­¾æ¥é¢„ä¼°æ‰©å®¹åçš„èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³å¾…è°ƒåº¦ Pod çš„è°ƒåº¦æ¡ä»¶ï¼ˆä¾‹å¦‚æ˜¯å¦åŒ¹é… NodeSelectorï¼‰ã€‚ä½†è¯·æ³¨æ„ï¼Œ**è¿™äº›æ ‡ç­¾å¹¶ä¸ä¼šçœŸæ­£åº”ç”¨åˆ°èŠ‚ç‚¹ä¸Š**ï¼Œå®ƒä»¬åªæ˜¯ç”¨æ¥ä¾› autoscaler åˆ¤æ–­æ˜¯å¦å€¼å¾—æ‰©å®¹ã€‚å®é™…åº”ç”¨æ ‡ç­¾æ˜¯åœ¨å¯åŠ¨èŠ‚ç‚¹æ—¶ç”± AWS çš„ UserData è„šæœ¬å®Œæˆçš„ã€‚



2.2 Pod Toleranz é…å¥—çš„ æ‰“åœ¨ node ä¸Šçš„ taint 
å¦‚æœæŸä¸ª ASG ä¸­çš„èŠ‚ç‚¹éœ€è¦å¸¦æœ‰ taintï¼ˆæ±¡ç‚¹ï¼‰ï¼Œé‚£ä¹ˆå¿…é¡»åœ¨è¯¥ ASG ä¸Šæ·»åŠ ç›¸åº”çš„ taint æ ‡ç­¾ï¼Œå¦åˆ™ Cluster Autoscaler å¯èƒ½ä¼šé”™è¯¯åœ°æ‰©å®¹å‡ºå¸¦æœ‰ taint çš„èŠ‚ç‚¹ï¼Œè€Œè¿™äº›èŠ‚ç‚¹åˆä¸èƒ½è¿è¡Œå¾…è°ƒåº¦çš„ Podï¼Œå¯¼è‡´è°ƒåº¦å¤±è´¥ã€‚
The following is only required if scaling up from 0 nodes. The Cluster Autoscaler will require the taint tag on the ASG, else tainted nodes may get spun up that cannot actually have the pending pods run on it. 
The tag is of the format `k8s.io/cluster-autoscaler/node-template/taint/<taint-name>`:`<taint-value:taint-effect>` is the name of the taint and the value of each tag specifies the taint value and effect with the format `<taint-value>:<taint-effect>`.

Example tags:
- `k8s.io/cluster-autoscaler/node-template/taint/dedicated`: `true:NoSchedule`

- `<taint-name>` æ˜¯æ±¡ç‚¹çš„åç§°
- `<taint-value>` æ˜¯æ±¡ç‚¹çš„å€¼
- `<taint-effect>` æ˜¯æ±¡ç‚¹çš„å½±å“ï¼ˆä¾‹å¦‚ `NoSchedule`, `PreferNoSchedule`, `NoExecute`ï¼‰

è¿™ä¸ªæ ‡ç­¾çš„å«ä¹‰æ˜¯ï¼šè¯¥èŠ‚ç‚¹ä¼šæœ‰ä¸€ä¸ªåä¸º `dedicated` çš„æ±¡ç‚¹ï¼Œå…¶å€¼ä¸º `true`ï¼Œæ•ˆæœä¸º `NoSchedule`ã€‚Cluster Autoscaler é€šè¿‡è¿™äº›æ ‡ç­¾æ¥**é¢„æµ‹**æ–°å»ºèŠ‚ç‚¹çš„å±æ€§ï¼Œä»è€Œå†³å®šæ˜¯å¦æ‰©å®¹ã€‚æ³¨æ„ï¼šå®é™…çš„ taint æ˜¯ç”± AWS çš„å¯åŠ¨è„šæœ¬ï¼ˆUserDataï¼‰åº”ç”¨åˆ°èŠ‚ç‚¹ä¸Šçš„ã€‚

**NOTE:** It is your responsibility to ensure such labels and/or taints are applied via the node's kubelet configuration at startup. Cluster Autoscaler will not set the node taints for you.
ä½ éœ€è¦è‡ªè¡Œç¡®ä¿é€šè¿‡èŠ‚ç‚¹å¯åŠ¨æ—¶çš„ kubelet é…ç½®æ¥åº”ç”¨è¿™äº›æ ‡ç­¾å’Œ/æˆ–æ±¡ç‚¹ï¼ˆtaintsï¼‰ã€‚Cluster Autoscaler ä¸ä¼šè‡ªåŠ¨ä¸ºä½ è®¾ç½®èŠ‚ç‚¹çš„æ±¡ç‚¹ã€‚
## 3.5 ç”¨ASG æ ‡ç­¾è¡¨æ˜è¿™ä¸ªASGä¸­èµ„æºçš„å¤§å°, å¸®åŠ© CA å†³ç­– 


è¿™äº›æ ‡ç­¾ä»…ç”¨äº **æ‰©å®¹å‰çš„æ¨¡æ‹Ÿè°ƒåº¦å†³ç­–** â€”â€” å®é™…çš„èµ„æºä»ç„¶ç”± EC2 å®ä¾‹ç±»å‹å†³å®šï¼Œè€Œæ ‡ç­¾åªæ˜¯å‘Šè¯‰ Cluster Autoscaler è¿™äº›èµ„æºçš„é¢„è®¡æ•°é‡ã€‚è¿™æ ·åœ¨ **ä» 0 èŠ‚ç‚¹æ‰©å®¹æ—¶**ï¼ŒAutoscaler æ‰èƒ½åˆ¤æ–­æ–°å»ºèŠ‚ç‚¹æ˜¯å¦æ»¡è¶³å¾…è°ƒåº¦ Pod çš„èµ„æºè¯·æ±‚ã€‚
From version 1.14, Cluster Autoscaler can also determine the resources provided by each Auto Scaling Group via tags. 

The tag is of the format  `k8s.io/cluster-autoscaler/node-template/resources/<resource-name>`.
`<resource-name>` is the name of the resource, such as `ephemeral-storage`. The value of each tag specifies the amount of resource provided. The units are identical to the units used in the `resources` field of a Pod specification.

Example tags:
- `k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage`: `100G`

- `<resource-name>` æ˜¯èµ„æºçš„åç§°ï¼Œä¾‹å¦‚ `ephemeral-storage`ï¼ˆä¸´æ—¶å­˜å‚¨ï¼‰ã€‚ è¿™ä¸ªç¤ºä¾‹è¡¨ç¤ºï¼šè¯¥ Auto Scaling Group ä¸­çš„èŠ‚ç‚¹æä¾› 100 GB çš„ä¸´æ—¶å­˜å‚¨ç©ºé—´ï¼ˆ`ephemeral-storage`ï¼‰ã€‚
- æ ‡ç­¾çš„å€¼è¡¨ç¤ºè¯¥èµ„æºçš„æ•°é‡ï¼Œå…¶å•ä½ä¸ Pod è§„èŒƒä¸­ `resources` å­—æ®µä¸­ä½¿ç”¨çš„å•ä½ç›¸åŒï¼ˆå¦‚ Miã€Giã€mã€G ç­‰ï¼‰ã€‚


## 3.6 ä½¿ç”¨ASG æ ‡ç­¾å¯ä»¥æŒ‡å®šè‡ªåŠ¨æ‰©ç¼©å®¹é€‰é¡¹ï¼Œ è¦†ç›–ä¸ºæ•´ä¸ª Cluster Autoscaler è®¾ç½®çš„å…¨å±€é…ç½®


ASG æ ‡ç­¾å¯ä»¥æŒ‡å®šè‡ªåŠ¨æ‰©ç¼©å®¹é€‰é¡¹ï¼Œä»è€Œè¦†ç›–ä¸ºæ•´ä¸ª Cluster Autoscaler è®¾ç½®çš„å…¨å±€é…ç½®ã€‚
ASG labels can specify autoscaling options, overriding the global cluster-autoscaler settings for the labeled ASGs. Those labels takes the same values format as the cluster-autoscaler command line flags they override (a float or a duration, encoded as string). Currently supported autoscaling options (and example values) are:
* `k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledownutilizationthreshold`: `0.5`
  (overrides `--scale-down-utilization-threshold` value for that specific ASG)
* `k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledowngpuutilizationthreshold`: `0.5`
  (overrides `--scale-down-gpu-utilization-threshold` value for that specific ASG)
* `k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledownunneededtime`: `10m0s`
  (overrides `--scale-down-unneeded-time` value for that specific ASG)
* `k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledownunreadytime`: `20m0s`
  (overrides `--scale-down-unready-time` value for that specific ASG)
* `k8s.io/cluster-autoscaler/node-template/autoscaling-options/ignoredaemonsetsutilization`: `true`
  (overrides `--ignore-daemonsets-utilization` value for that specific ASG)

`--scale-down-utilization-threshold` æ˜¯åŠ åœ¨ è¿™é‡Œçš„ https://github.com/kubernetes/autoscaler/blob/3d748040d9951522c5445abbf6fa379c5ec71ed2/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml#L165






## 3.7 Recommendations:
- It is recommended to use a second tag like `k8s.io/cluster-autoscaler/<cluster-name>` when `k8s.io/cluster-autoscaler/enabled` is used across many clusters to prevent ASGs from different clusters having conflicts.
  An ASG must contain at least all the tags specified and as such secondary tags can differentiate between different clusters ASGs.
- To prevent conflicts, do not provide a `--nodes` argument if `--node-group-auto-discovery` is specified.
- Be sure to add `autoscaling:DescribeLaunchConfigurations` or `ec2:DescribeLaunchTemplateVersions` to the `Action` list of the IAM Policy used by Cluster Autoscaler, depending on whether your ASG utilizes Launch Configurations or Launch Templates.
- If Cluster Autoscaler adds a node to the cluster, and the node has taints applied when it joins the cluster that Cluster Autoscaler was unaware of (because the tag wasn't supplied in ASG), this can lead to significant confusion and misbehaviour.

- å¦‚æœåœ¨å¤šä¸ªé›†ç¾¤ä¸­éƒ½ä½¿ç”¨äº† `k8s.io/cluster-autoscaler/enabled` æ ‡ç­¾ï¼Œå»ºè®®å†åŠ ä¸Šä¸€ä¸ª `k8s.io/cluster-autoscaler/<cluster-name>` æ ‡ç­¾æ¥åŒºåˆ†ä¸åŒé›†ç¾¤çš„ ASGï¼Œé¿å…å†²çªã€‚
- å¦‚æœå¯ç”¨äº† `--node-group-auto-discovery`ï¼Œè¯·ä¸è¦å†è®¾ç½® `--nodes` å‚æ•°ï¼Œä»¥é¿å…é…ç½®å†²çªã€‚
- è¯·ç¡®ä¿å°† `autoscaling:DescribeLaunchConfigurations` æˆ– `ec2:DescribeLaunchTemplateVersions` åŠ å…¥åˆ° Cluster Autoscaler æ‰€ä½¿ç”¨çš„ IAM Policy çš„ `Action` åˆ—è¡¨ä¸­ï¼Œå…·ä½“å–å†³äºä½ çš„ ASG æ˜¯ä½¿ç”¨ Launch Configuration è¿˜æ˜¯ Launch Templateã€‚    
- å¦‚æœ Cluster Autoscaler å‘é›†ç¾¤æ·»åŠ èŠ‚ç‚¹ï¼Œè€Œè¯¥èŠ‚ç‚¹åœ¨åŠ å…¥é›†ç¾¤æ—¶å¸¦æœ‰æœªè¢« ASG æ ‡ç­¾å£°æ˜çš„æ±¡ç‚¹ï¼Œè¿™å¯èƒ½å¯¼è‡´è°ƒåº¦è¡Œä¸ºå¼‚å¸¸æˆ–ä»¤äººå›°æƒ‘ã€‚



## 3.8 Special note on GPU instances

èŠ‚ç‚¹ä¸Šæä¾› GPU èµ„æºçš„è®¾å¤‡æ’ä»¶éœ€è¦ä¸€äº›æ—¶é—´æ‰èƒ½å°† GPU èµ„æºé€šå‘Šç»™é›†ç¾¤ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ Cluster Autoscaler å¤šæ¬¡ä¸å¿…è¦åœ°è¿›è¡Œæ‰©å®¹ã€‚
The device plugin on nodes that provides GPU resources can take some time to advertise the GPU resource to the cluster. This may cause Cluster Autoscaler to unnecessarily scale out multiple times.

To avoid this, you can configure `kubelet` on your GPU nodes to label the node before it joins the cluster by passing it the `--node-labels` flag. The label format is as follows:
- Cluster Autoscaler < 1.15: `cloud.google.com/gke-accelerator=<gpu-type>`
- Cluster Autoscaler >= 1.15: `k8s.amazonaws.com/accelerator=<gpu-type>`

`<gpu-type>` varies by instance type. On P2 instances, for example, the value is `nvidia-tesla-k80`.


# 4 ASG ä¸Šæ ‡ç­¾çš„ä¾‹å­ 


- ä½ æœ‰ä¸€ä¸ª EKS é›†ç¾¤åä¸º `my-eks-cluster`
- ä½ æƒ³è®© Cluster Autoscaler èƒ½å‘ç°è¿™ä¸ª ASG å¹¶æ­£ç¡®æ¨æ–­å…¶èŠ‚ç‚¹çš„ labelsã€taintsã€èµ„æºç­‰ä¿¡æ¯
- åŒæ—¶ä½ æƒ³ä¸ºè¿™ä¸ª ASG é…ç½®è‡ªå®šä¹‰çš„ scale down ç­–ç•¥

å¿…éœ€çš„ ASG æ ‡ç­¾ï¼ˆåŸºç¡€è‡ªåŠ¨å‘ç°ï¼‰

| Key                                        | Value   | è¯´æ˜                    |
| ------------------------------------------ | ------- | --------------------- |
| `k8s.io/cluster-autoscaler/enabled`        | `true`  | å¯ç”¨è‡ªåŠ¨å‘ç°ï¼ˆå€¼å¯ä»¥ä»»æ„ï¼‰         |
| `k8s.io/cluster-autoscaler/my-eks-cluster` | `owned` | æŒ‡æ˜å±äºå“ªä¸ªé›†ç¾¤ï¼ˆå€¼ä»»æ„ï¼Œä½†å»ºè®®ä¿æŒä¸€è‡´ï¼‰ |

å¯é€‰æ ‡ç­¾ï¼šNode Labels / Taints / Resources / Autoscaling Options

| Key                                                                                         | Value             | ç¤ºä¾‹è¯´æ˜                           |
| ------------------------------------------------------------------------------------------- | ----------------- | ------------------------------ |
| `k8s.io/cluster-autoscaler/node-template/label/node-type`                                   | `spot`            | è®¾ç½®èŠ‚ç‚¹æ ‡ç­¾ node-type=spot          |
| `k8s.io/cluster-autoscaler/node-template/taint/dedicated`                                   | `true:NoSchedule` | è®¾ç½®æ±¡ç‚¹ dedicated=true:NoSchedule |
| `k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage`                       | `100Gi`           | è®¾ç½®æœ¬åœ°ä¸´æ—¶å­˜å‚¨å®¹é‡                     |
| `k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledownutilizationthreshold` | `0.4`             | å•ç‹¬ä¸ºè¯¥ ASG è®¾ç½®æ›´æ¿€è¿›çš„ç¼©å®¹é˜ˆå€¼            |
| `k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledownunneededtime`         | `5m`              | ç¼©å®¹å‰ç­‰å¾…æ—¶é—´ç¼©çŸ­ä¸º 5 åˆ†é’Ÿ                |

```yaml
resource "aws_autoscaling_group" "example" {
  name                      = "example-asg"
  min_size                  = 0
  max_size                  = 5
  desired_capacity          = 0
  vpc_zone_identifier       = ["subnet-xxx"]
  launch_template {
    id      = aws_launch_template.example.id
    version = "$Latest"
  }

  tags = [
    {
      key                 = "k8s.io/cluster-autoscaler/enabled"
      value               = "true"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/my-eks-cluster"
      value               = "owned"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/node-template/label/node-type"
      value               = "spot"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/node-template/taint/dedicated"
      value               = "true:NoSchedule"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/node-template/autoscaling-options/scaledownutilizationthreshold"
      value               = "0.4"
      propagate_at_launch = true
    }
  ]
}


```


- æ‰€æœ‰æ ‡ç­¾å¿…é¡»è®¾ç½® `propagate_at_launch = true`ï¼Œå¦åˆ™æ–°èŠ‚ç‚¹ä¸ä¼šç»§æ‰¿è¿™äº›æ ‡ç­¾ã€‚    
- å¦‚æœä½ ä½¿ç”¨ Launch Templateï¼Œè¯·ç¡®ä¿ `userData` é‡Œæ­£ç¡®æ³¨å…¥äº†èŠ‚ç‚¹æ ‡ç­¾å’Œæ±¡ç‚¹ï¼Œå¦åˆ™æ ‡ç­¾åªæ˜¯è®© Cluster Autoscaler èƒ½è¯†åˆ«ï¼Œä½†å®é™…èŠ‚ç‚¹å¹¶ä¸ä¼šæ‹¥æœ‰è¿™äº›å±æ€§ã€‚



## 4.1 propagate_at_launch = true çš„æ„ä¹‰


åœ¨ Auto Scaling Groupï¼ˆASGï¼‰ä¸­ï¼Œä½ å¯ä»¥ä¸º ASG è®¾ç½®æ ‡ç­¾ã€‚ä½†é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™äº›æ ‡ç­¾åªå­˜åœ¨äº ASG æœ¬èº«ï¼Œå¹¶ä¸ä¼šè‡ªåŠ¨ä¼ é€’ç»™è¯¥ ASG å¯åŠ¨çš„ EC2 å®ä¾‹ã€‚

è¦è®©æŸä¸ªæ ‡ç­¾ åœ¨å®ä¾‹å¯åŠ¨æ—¶è‡ªåŠ¨é™„åŠ åˆ° EC2 å®ä¾‹ä¸Šï¼Œä½ å¿…é¡»è®¾ç½®ï¼š propagate_at_launch = true


æ ‡ç­¾åªå­˜åœ¨äº ASGï¼Œä¸ä¼šå‡ºç°åœ¨ EC2 å®ä¾‹ä¸Šã€‚
å¯¹äºä¸€äº›ä¾èµ– EC2 å®ä¾‹æ ‡ç­¾çš„å·¥å…·ï¼ˆå¦‚ CloudWatchã€IAM æ¡ä»¶ç­–ç•¥ã€æŸäº›è‡ªåŠ¨åŒ–è„šæœ¬ï¼‰æ¥è¯´ï¼Œå®ƒä»¬å¯èƒ½å°±æ— æ³•è¯†åˆ«è¿™äº›æ ‡ç­¾ã€‚


| æœºåˆ¶                               | ä½œç”¨å¯¹è±¡            | æ˜¯å¦å½±å“ Kubernetes Node çš„æ ‡ç­¾/æ±¡ç‚¹      | ç”¨é€”è¯´æ˜                                            |
| -------------------------------- | --------------- | -------------------------------- | ----------------------------------------------- |
| `propagate_at_launch = true`     | AWS å±‚çš„ EC2 å®ä¾‹æ ‡ç­¾ | âŒ ä¸ä¼šå½±å“ Kubernetes Node æ ‡ç­¾æˆ– taint | åªæ˜¯è®© **EC2 å®ä¾‹ç»§æ‰¿ ASG æ ‡ç­¾**ï¼Œå¯¹ K8s èŠ‚ç‚¹è¡Œä¸ºæ— ç›´æ¥å½±å“         |
| **Launch Template çš„ `userData`** | èŠ‚ç‚¹å¯åŠ¨æ—¶æ‰§è¡Œçš„è„šæœ¬      | âœ… ç›´æ¥å†³å®šèŠ‚ç‚¹æ˜¯å¦å¸¦æœ‰ label / taint       | å¿…é¡»åœ¨ userData é‡Œæ³¨å…¥ç›¸å…³ kubelet å‚æ•°æˆ–è„šæœ¬ï¼Œæ‰èƒ½è®© Node è¢«æ­£ç¡®æ ‡è®° |

`propagate_at_launch = true` è¿™ä¸ªæ ‡ç­¾ä¼šå‡ºç°åœ¨ **EC2 å®ä¾‹æ ‡ç­¾ä¸Š**ï¼Œä½† K8s ä¸­çš„ `kubectl get nodes --show-labels` å¹¶ä¸ä¼šçœ‹åˆ° `node-type=gpu`ã€‚Cluster Autoscaler åªä¼šæ ¹æ®è¿™ä¸ªæ ‡ç­¾**é¢„ä¼°æ‰©å®¹æ—¶è¦é€‰å“ªä¸ª ASG**ï¼Œä¸ä¼šä¿®æ”¹ä»»ä½• Node æ ‡ç­¾ã€‚


----

åªæ˜¯è®© EC2 å®ä¾‹ç»§æ‰¿ ASG æ ‡ç­¾ï¼Œå¯¹ K8s èŠ‚ç‚¹è¡Œä¸ºæ— ç›´æ¥å½±å“   é‚£ä¹ˆ è®© EC2 å®ä¾‹ç»§æ‰¿ ASG æ ‡ç­¾çš„æ„ä¹‰æ˜¯ä»€ä¹ˆ 

è®© EC2 å®ä¾‹ç»§æ‰¿ ASG æ ‡ç­¾ï¼ˆ`propagate_at_launch = true`ï¼‰çš„æ„ä¹‰ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š
1. **æ–¹ä¾¿ AWS å±‚èµ„æºç®¡ç†å’Œè¯†åˆ«**  
    ASG æ ‡ç­¾ä¼šè‡ªåŠ¨ä¼ é€’ç»™ EC2 å®ä¾‹ï¼Œæ–¹ä¾¿ä½ åœ¨ AWS æ§åˆ¶å°æˆ–è€…é€šè¿‡ API è¯†åˆ«ã€ç­›é€‰å±äºå“ªä¸ª ASG çš„å®ä¾‹ã€‚  
    ä¾‹å¦‚ï¼Œä½ å¯ä»¥é€šè¿‡æ ‡ç­¾å¿«é€ŸæŸ¥è¯¢æ‰€æœ‰å±äºæŸä¸ª ASG çš„å®ä¾‹ï¼Œæ–¹ä¾¿ç›‘æ§ã€è®¡è´¹å’Œç®¡ç†ã€‚
    
2. **æƒé™å’Œç­–ç•¥åº”ç”¨**  
    æœ‰äº› AWS IAM ç­–ç•¥æˆ–è‡ªåŠ¨åŒ–å·¥å…·ï¼Œå¯èƒ½ä¼šæ ¹æ® EC2 å®ä¾‹æ ‡ç­¾æ¥æˆäºˆæƒé™æˆ–åšå…¶ä»–æ“ä½œã€‚  
    ä¾‹å¦‚ï¼ŒåŸºäºæ ‡ç­¾çš„æƒé™æ§åˆ¶ (IAM Conditions) å¯ä»¥é™åˆ¶æŸäº›å®ä¾‹æ‰§è¡Œç‰¹å®šæ“ä½œã€‚
    
3. **è‡ªåŠ¨åŒ–è„šæœ¬å’Œè¿ç»´å·¥å…·ä½¿ç”¨**  
    è®¸å¤šè‡ªåŠ¨åŒ–å·¥å…·ï¼ˆç›‘æ§ã€å¤‡ä»½ã€é…ç½®ç®¡ç†ï¼‰ä¼šæ ¹æ® EC2 å®ä¾‹æ ‡ç­¾æ¥å†³å®šæ‰§è¡Œå“ªäº›ä»»åŠ¡ï¼Œç»§æ‰¿æ ‡ç­¾åèƒ½ç¡®ä¿å·¥å…·æ­£å¸¸è¯†åˆ«å®ä¾‹å±æ€§ã€‚
    
4. **Cluster Autoscaler ç”¨äºè¯†åˆ« ASG**  
    Cluster Autoscaler é€šè¿‡æŸ¥çœ‹ ASG æ ‡ç­¾æ¥çŸ¥é“å®ƒç®¡ç†çš„å“ªäº› ASGï¼Œè™½ç„¶æ ‡ç­¾ä¸ä¼šç›´æ¥å½±å“ Kubernetes èŠ‚ç‚¹æ ‡ç­¾ï¼Œä½† Cluster Autoscaler ä¾èµ–è¿™äº›æ ‡ç­¾ä¿¡æ¯æ¥åˆ¤æ–­å¦‚ä½•æ‰©ç¼©å®¹ã€‚


- **AWS å®ä¾‹æ ‡ç­¾æ˜¯ AWS èµ„æºå±‚é¢çš„ç®¡ç†æ ‡è¯†**ï¼Œæ–¹ä¾¿ä½ åœ¨ AWS ç”Ÿæ€å†…ç®¡ç†å’Œæ“ä½œå®ä¾‹ã€‚
- **Kubernetes èŠ‚ç‚¹æ ‡ç­¾/æ±¡ç‚¹æ˜¯é›†ç¾¤å†…éƒ¨çš„è°ƒåº¦å’Œç®¡ç†æ ‡è¯†**ï¼Œç”¨äº Pod è°ƒåº¦ç­‰é€»è¾‘ã€‚
- ä¸¤è€…æœåŠ¡äºä¸åŒå±‚é¢ï¼Œæ ‡ç­¾ç»§æ‰¿æ˜¯ AWS èµ„æºç®¡ç†çš„åŸºç¡€æ“ä½œï¼Œä¸å½±å“ Kubernetes å†…éƒ¨è¡Œä¸ºã€‚




## 4.2 ä½ ä½¿ç”¨ Launch Template/Congurationï¼Œè¯·ç¡®ä¿ userData é‡Œæ­£ç¡®æ³¨å…¥äº†èŠ‚ç‚¹æ ‡ç­¾å’Œæ±¡ç‚¹, æ‰ä¼šå°† çœŸçš„æŠŠè¿™äº›å±æ€§â€œè®¾ç½®â€åˆ°æ–°èŠ‚ç‚¹ä¸Šã€‚

ä½ åœ¨ Auto Scaling Groupï¼ˆASGï¼‰é‡Œæ·»åŠ äº†è¿™æ ·çš„æ ‡ç­¾ï¼š
k8s.io/cluster-autoscaler/node-template/label/node-type = spot
k8s.io/cluster-autoscaler/node-template/taint/dedicated = true:NoSchedule

è¿™ä¼šå‘Šè¯‰ **Cluster Autoscaler**ï¼š

> â€œå˜¿ï¼Œè¿™ä¸ª ASG ä¸­çš„èŠ‚ç‚¹åº”è¯¥æœ‰ `node-type=spot` æ ‡ç­¾ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ª `dedicated=true:NoSchedule` çš„ taintï¼Œæ‰€ä»¥å¦‚æœæœ‰ Pod éœ€è¦è¿™äº›å±æ€§ï¼Œæˆ‘å°±å¯ä»¥æ‰©å®¹è¿™ç»„èŠ‚ç‚¹ã€‚â€

ä½†å®é™…ä¸Š â€”â€” Cluster **Autoscaler ä¸ä¼šçœŸæ­£å»è®¾ç½®è¿™äº›æ ‡ç­¾æˆ– taint åˆ°æ–°èŠ‚ç‚¹ä¸Š**ã€‚å®ƒåªæ˜¯â€œç›¸ä¿¡â€è¿™äº›æ–°èŠ‚ç‚¹åœ¨å¯åŠ¨æ—¶å·²ç»å…·å¤‡è¿™äº›å±æ€§ã€‚


---

ä½ è‡ªå·±åœ¨ Launch Template çš„ userDataï¼ˆå¯åŠ¨è„šæœ¬ï¼‰é‡Œè®¾ç½®ï¼

æ¯”å¦‚é€šè¿‡ kubelet çš„å¯åŠ¨å‚æ•°æŒ‡å®šï¼š

--node-labels=node-type=spot
--register-with-taints=dedicated=true:NoSchedule

è¿™äº›å‚æ•°é€šå¸¸å†™åœ¨ Launch Template çš„ `userData` éƒ¨åˆ†ä¸­ï¼ˆbase64 ç¼–ç çš„è„šæœ¬ï¼‰ï¼Œç”¨äºå®ä¾‹å¯åŠ¨æ—¶è‡ªåŠ¨æ³¨å…¥åˆ°èŠ‚ç‚¹ã€‚


---

 å¦‚æœä½ æ²¡æœ‰åœ¨ userData ä¸­è®¾ç½®è¿™äº›å‚æ•°ä¼šæ€æ ·ï¼Ÿ

- èŠ‚ç‚¹å®é™…ä¸Š æ²¡æœ‰ node-type=spot æ ‡ç­¾ï¼Œ
- èŠ‚ç‚¹ä¹Ÿ æ²¡æœ‰ dedicated=true:NoSchedule çš„ taintï¼Œ
- ä½†æ˜¯ Cluster Autoscaler ä»¥ä¸ºå®ƒä»¬æœ‰ï¼
- ç»“æœï¼šPod ä»ç„¶è°ƒåº¦ä¸ä¸Šï¼Œæ‰©å®¹ç™½åšäº†ï¼Œç³»ç»Ÿå‡ºç°â€œè¯¯åˆ¤â€ã€‚

---

ç¤ºä¾‹ï¼šTerraform é…ç½® Launch Template çš„æ–¹å¼ï¼ˆåŒ…å«è®¾ç½®èŠ‚ç‚¹æ ‡ç­¾å’Œ taintï¼‰
```yaml


resource "aws_launch_template" "example" {
  name_prefix   = "example-"
  image_id      = "ami-12345678" # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ AMI ID
  instance_type = "t3.medium"

  # ä½ éœ€è¦åœ¨ `userData` ä¸­ä¼ é€’ kubelet å¯åŠ¨å‚æ•°ï¼Œè®©æ–° EC2 èŠ‚ç‚¹å¯åŠ¨åï¼Œkubelet è¿è¡Œæ—¶è‡ªåŠ¨æ·»åŠ æ ‡ç­¾å’Œæ±¡ç‚¹ã€‚
  user_data = base64encode(<<-EOF
    #!/bin/bash
    set -ex

    # å®‰è£…å¿…è¦çš„å·¥å…·ï¼Œå¦‚ awscliã€kubelet ç­‰ï¼ˆç•¥ï¼‰

    # å¯åŠ¨ kubeletï¼Œæ³¨å…¥æ ‡ç­¾å’Œ taint
    /etc/eks/bootstrap.sh ${cluster_name} \
      --kubelet-extra-args "--node-labels=node-type=spot,env=dev --register-with-taints=dedicated=true:NoSchedule"
  EOF
  )

  # å¦‚æœä½ ä½¿ç”¨ Launch Template ä¸ EKS ç»“åˆï¼Œè¿˜å¯ä»¥é…ç½®ä»¥ä¸‹å†…å®¹ï¼š
  lifecycle {
    create_before_destroy = true
  }

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name = "example-node"
    }
  }

  tag_specifications {
    resource_type = "volume"
    tags = {
      Name = "example-node-volume"
    }
  }
}


```




----

ä½¿ç”¨  launch_template  é…åˆ Cluster Autoscaler çš„æ ‡ç­¾ï¼ˆè®¾ç½®åœ¨ ASG ä¸Šï¼‰ï¼š
Cluster Autoscaler ä¾èµ–ä»¥ä¸‹æ ‡ç­¾è¯†åˆ«èŠ‚ç‚¹ç»„ï¼š
å¦‚æœä½ ä½¿ç”¨ eks_node_group æ¨¡å—ï¼ˆEKS æ‰˜ç®¡èŠ‚ç‚¹ç»„ï¼‰ï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ launch_template å—ä¼ å…¥è¿™ä¸ª Launch Template IDã€‚
```yaml
resource "aws_autoscaling_group" "example" {
  name_prefix = "example-asg"
  ...

  tag {
    key                 = "k8s.io/cluster-autoscaler/enabled"
    value               = "true"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/${cluster_name}"
    value               = "owned"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/node-template/label/node-type"
    value               = "spot"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/node-template/taint/dedicated"
    value               = "true:NoSchedule"
    propagate_at_launch = true
  }
}

```


---


ç”¨ Launch Configuration è®¾ç½®èŠ‚ç‚¹æ ‡ç­¾å’Œ taint çš„æ–¹æ³•

âš ï¸ Launch Configuration å’Œ Launch Template çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ï¼šLaunch Configuration æ˜¯æ—§ç‰ˆæœºåˆ¶ï¼Œä¸æ”¯æŒåƒ Launch Template é‚£æ ·ä½¿ç”¨æ›´çµæ´»çš„å‚æ•°ï¼ˆå¦‚ç‰ˆæœ¬ã€å¤šä¸ªç½‘ç»œæ¥å£ç­‰ï¼‰ã€‚å»ºè®®ä¼˜å…ˆä½¿ç”¨ Launch Templateï¼Œä½†å¦‚æœä½ å¿…é¡»ä½¿ç”¨ Launch Configurationï¼Œä¹Ÿå¯ä»¥å®ç°èŠ‚ç‚¹æ ‡ç­¾å’Œæ±¡ç‚¹æ³¨å…¥ã€‚


```
resource "aws_launch_configuration" "example" {
  name_prefix   = "eks-node-"
  image_id      = data.aws_ami.eks_worker.id
  instance_type = "t3.large"
  key_name      = var.ssh_key_name
  iam_instance_profile = aws_iam_instance_profile.eks_nodes.name
  security_groups      = [aws_security_group.eks_nodes.id]

  user_data = base64encode(<<-EOF
    #!/bin/bash
    /etc/eks/bootstrap.sh ${var.cluster_name} \
      --kubelet-extra-args '--node-labels=node-type=spot --register-with-taints=dedicated=true:NoSchedule'
  EOF
  )

  lifecycle {
    create_before_destroy = true
  }
}

```



---

æ ¸å¿ƒåŒºåˆ«ï¼šLaunch Configuration vs Launch Template
Launch Configuration è™½ç„¶è¿˜åœ¨æ”¯æŒï¼Œä½†ï¼š
âš ï¸ AWS æ—©å·²å»ºè®®è¿ç§»åˆ° Launch Templateï¼Œå¹¶ä¸”æŸäº›æ–°åŠŸèƒ½åªèƒ½é€šè¿‡ Launch Template ä½¿ç”¨ã€‚

| åŠŸèƒ½/ç‰¹æ€§                                       | **Launch Configuration** | **Launch Template**                                                                                                                   |
| ------------------------------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |
| ğŸ”„ **ç‰ˆæœ¬ç®¡ç†**                                 | âŒ ä¸æ”¯æŒ                    | âœ… **æ”¯æŒç‰ˆæœ¬æ§åˆ¶**ï¼ˆå¯ä»¥ä¿ç•™å¤šä¸ªç‰ˆæœ¬å¹¶æŒ‡å®šé»˜è®¤ç‰ˆæœ¬ï¼‰                                                                                                         |
| ğŸ–¥ï¸ **å¤šä¸ªç½‘ç»œæ¥å£**                              | âŒ ä¸æ”¯æŒ                    | âœ… æ”¯æŒå¤šä¸ª ENIï¼ˆå¼¹æ€§ç½‘ç»œæ¥å£ï¼‰é…ç½®                                                                                                                  |
| ğŸ§¾ **æ”¯æŒ Launch Template Overridesï¼ˆæ··åˆå®ä¾‹ç­–ç•¥ï¼‰** | âŒ ä¸æ”¯æŒ                    | âœ… **å¿…é¡»ä½¿ç”¨ Launch Template**                                                                                                            |
| ğŸ’¾ **EBS å·é…ç½®çµæ´»æ€§**                           | ğŸš« æœ‰é™                    | âœ… æ›´çµæ´»ã€æ”¯æŒé«˜çº§é€‰é¡¹ï¼ˆå¦‚åŠ å¯†ã€åˆ é™¤ç­–ç•¥ï¼‰                                                                                                                |
| ğŸ“ **User Data åŠ¨æ€å‚æ•°æ”¯æŒ**                     | âŒ åªèƒ½é™æ€å†…å®¹                 | âœ… æ”¯æŒ [Instance Metadata substitution](https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html#template-user-data) |
| ğŸ§© **GPU ç­‰èµ„æºç±»å‹æ”¯æŒ**                          | ğŸš« æ—§èµ„æºæ”¯æŒè¾ƒå·®               | âœ… æ¨èç”¨äº **GPU/åŠ é€Ÿå‹å®ä¾‹ã€Spot é…ç½®**                                                                                                          |
| ğŸš€ **æœªæ¥æ”¯æŒ**                                 | âŒ AWS å·²ä¸å†å»ºè®®ä½¿ç”¨            | âœ… **AWS æ¨èæ–°é¡¹ç›®ä½¿ç”¨ Launch Template**                                                                                                     |





# 5 Manual configuration: é™å®šASG çš„max min node number 

Cluster Autoscaler can also be configured manually if you wish by passing the `--nodes` argument at startup. 
The format of the argument is `--nodes=<min>:<max>:<asg-name>`, where `<min>` is the minimum number of nodes, `<max>` is the maximum number of nodes, and `<asg-name>` is the Auto Scaling Group name.

You can pass multiple `--nodes` arguments if you have multiple Auto Scaling Groups you want Cluster Autoscaler to use.

**NOTES**:

- Both `<min>` and `<max>` must be within the range of the minimum and maximum instance counts specified by the Auto Scaling group.
- When manual configuration is used, all Auto Scaling groups must use EC2 instance types that provide equal CPU and memory capacity.

Examples:
One ASG Setup (min: 1, max: 10, ASG Name: k8s-worker-asg-1)
å¯ä»¥åœ¨å¯åŠ¨ Cluster Autoscaler æ—¶åŠ ä¸Šï¼š `--nodes=1:10:k8s-worker-asg-1`

åœ¨ Kubernetes ä¸­å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼éƒ¨ç½²é…ç½®æ–‡ä»¶ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```
kubectl apply -f examples/cluster-autoscaler-one-asg.yaml
```


```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities", "volumeattachments"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.1
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes=1:10:k8s-worker-asg-1
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt # /etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
```



----

Multiple ASG Setup
å¦‚æœä½ å¸Œæœ› Cluster Autoscaler åŒæ—¶ç®¡ç†å¤šä¸ª ASGï¼Œä½ å¯ä»¥é‡å¤åŠ  --nodes å‚æ•°ï¼Œæ¯”å¦‚ï¼š
```
--nodes=1:5:k8s-worker-spot-asg \
--nodes=2:8:k8s-worker-on-demand-asg
```

éƒ¨ç½²æ—¶ï¼Œç¤ºä¾‹ YAML æ–‡ä»¶å¯èƒ½å¦‚ä¸‹ï¼š
```
kubectl apply -f examples/cluster-autoscaler-multi-asg.yaml
```


```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities", "volumeattachments"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.1
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --nodes=1:10:k8s-worker-asg-1
            - --nodes=1:3:k8s-worker-asg-2
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt # /etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
```




---

æ‰‹åŠ¨é…ç½®æ—¶ï¼Œä½ å¿…é¡»ç¡®ä¿ min å’Œ max éƒ½åœ¨ ASG çš„å®é™…è®¾ç½®èŒƒå›´ä¹‹å†…ï¼ˆå³ Auto Scaling Group æ§å°é‡Œè®¾ç½®çš„æœ€å°å’Œæœ€å¤§å®ä¾‹æ•°ï¼‰ã€‚

æ‰€æœ‰è¢«æ‰‹åŠ¨é…ç½®çš„ ASGï¼Œå¿…é¡»ä½¿ç”¨ç›¸åŒè§„æ ¼çš„å®ä¾‹ç±»å‹ï¼ˆå³ CPU å’Œå†…å­˜ä¸€è‡´ï¼‰ï¼Œå¦åˆ™ CA æ— æ³•æ­£ç¡®åšå‡ºæ‰©ç¼©å®¹å†³ç­–ã€‚

å¦‚æœä½ ä½¿ç”¨è¿™ç§æ‰‹åŠ¨ --nodes é…ç½®æ–¹å¼ï¼Œå°±ä¸è¦ä½¿ç”¨è‡ªåŠ¨å‘ç°ï¼ˆ--node-group-auto-discoveryï¼‰ï¼Œé¿å…å†²çªã€‚


# 6 Control Plane (previously referred to as master) Node Setup

**NOTE**: This setup is not compatible with Amazon EKS.

To run a CA pod on a control plane node the CA deployment should tolerate the `master` taint and `nodeSelector` should be used to schedule the pods on a control plane node.

Please replace `{{ node_asg_min }}`, `{{ node_asg_max }}` and `{{ name }}` with your ASG setting in the yaml file.

```
kubectl apply -f examples/cluster-autoscaler-run-on-control-plane.yaml
```

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities", "volumeattachments"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      tolerations:
        - effect: NoSchedule
          operator: "Equal"
          value: "true"
          key: node-role.kubernetes.io/control-plane
      nodeSelector:
        kubernetes.io/role: control-plane
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.1
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes={{ node_asg_min }}:{{ node_asg_max }}:{{ name }}
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt # /etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
```


# 7 Using Mixed Instances Policies and Spot Instances

https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-properties-autoscaling-autoscalinggroup-mixedinstancespolicy.html

**NOTE:** The minimum version of cluster autoscaler to support MixedInstancePolicy is v1.14.x.

If your workloads can tolerate interruption, consider taking advantage of Spot Instances for a lower price point.

To enable diversity among On Demand and Spot Instances, as well as specify multiple EC2 instance types in order to tap into multiple Spot capacity pools, use a [mixed instances policy](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-mixedinstancespolicy.html) on your ASG. ==Note that the instance types should have the same amount of RAM and number of CPU cores, since this is fundamental to CA's scaling calculations==.  æ³¨æ„ï¼šä½ æ‰€æŒ‡å®šçš„å®ä¾‹ç±»å‹**åº”å…·æœ‰ç›¸åŒçš„å†…å­˜å’Œ CPU æ ¸å¿ƒæ•°é‡**ï¼Œå› ä¸ºè¿™æ˜¯ Cluster Autoscaler æ‰§è¡Œæ‰©ç¼©å®¹è®¡ç®—çš„åŸºç¡€.   Using mismatched instances types can produce unintended results. See an example below.

![](image/Pasted%20image%2020250727173338.png)


for example:

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: beta.kubernetes.io/instance-type
                operator: In
                values:
                  - r5.2xlarge
                  - r5d.2xlarge
                  - r5a.2xlarge
                  - r5ad.2xlarge
                  - r5n.2xlarge
                  - r5dn.2xlarge
                  - r4.2xlarge
                  - i3.2xlarge
```



1 nodeSelector ä¸å¤ªåˆé€‚
è¿˜æœ‰å…¶ä»–ä¼šå½±å“æ‰©ç¼©å®¹è¡Œä¸ºçš„å› ç´ ï¼Œä¾‹å¦‚ èŠ‚ç‚¹æ ‡ç­¾ï¼ˆnode labelsï¼‰ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ nodeSelector å¹¶ä¾èµ– beta.kubernetes.io/instance-type æ ‡ç­¾æ¥è°ƒåº¦ Podï¼Œé‚£ä¹ˆä½ åº”è¯¥ä¸º ASG æ·»åŠ ä¸€ä¸ªç»Ÿä¸€ä¼ æ’­çš„æ ‡ç­¾æ¥æ›¿ä»£å®ƒï¼Œå› ä¸ºè¿™ä¸ª instance-type æ ‡ç­¾ä¸å†å¯é ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„ ASG ä¸­æœ‰å¤šä¸ªä¸åŒçš„å®ä¾‹ç±»å‹ï¼Œè¿™ä¸ªæ ‡ç­¾æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸ä¸€æ ·ï¼Œå°±æ— æ³•ä½œä¸ºç»Ÿä¸€è°ƒåº¦ä¾æ®ã€‚
Additionally, there are other factors which affect scaling, such as node labels. If you are currently using `nodeSelector` with the
[beta.kubernetes.io/instance-type](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels) label, you will need to apply a common propagating label to the ASG and use that instead, since the instance-type label can no longer be relied upon. One may also use auto-generated tags such as `aws:cloudformation:stack-name` for this purpose. 

2 nodeAffinity åˆé€‚ 
è€Œ èŠ‚ç‚¹äº²å’Œæ€§ï¼ˆnodeAffinityï¼‰å’Œåäº²å’Œæ€§ï¼ˆanti-affinityï¼‰ ä¸ä¼šå—è¿™ä¸ªé—®é¢˜å½±å“ï¼Œå› ä¸ºå®ƒä»¬æœ¬èº«å°±æ”¯æŒå¤šä¸ªå€¼ã€‚
[Node affinity and anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) are not affected in the same way, since these selectors natively accept multiple values; one must add all the configured instances types to the list of values


3 --balancing-label å‚æ•°
åŒæ ·åœ°ï¼Œå¦‚æœä½ ä½¿ç”¨äº† --balancing-label å‚æ•°ï¼ˆç”¨äºåœ¨èŠ‚ç‚¹é—´åšè´Ÿè½½å‡è¡¡ï¼‰ï¼Œä½ å¿…é¡»ç¡®ä¿ä½¿ç”¨çš„æ ‡ç­¾åœ¨è¯¥èŠ‚ç‚¹ç»„çš„æ‰€æœ‰èŠ‚ç‚¹ä¸Šå…·æœ‰ç›¸åŒçš„å€¼ã€‚å¦åˆ™ï¼Œç”±äº ASG ä¸­çš„ä¸åŒå®ä¾‹å¯èƒ½ä¼šæ‹¥æœ‰ä¸åŒçš„æ ‡ç­¾å€¼ï¼Œå°±ä¼šå¯¼è‡´ **Cluster Autoscaler è¡Œä¸ºå¼‚å¸¸**ã€‚
Similarly, if using the `balancing-label` flag, you should only choose labels which have the same value for all nodes in the node group.  Otherwise you may get unexpected results, as the flag values will vary based on the nodes created by the ASG.

## 7.1 Example usage

- Create a [Launch  Template](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-launchtemplate.html) (LT) with an instance type, for example, r5.2xlarge. Consider this the 'base' instance type. Do not define any spot purchase options here.
    - æ­¤æ¨¡æ¿ä¼šå®šä¹‰å¯åŠ¨è„šæœ¬ï¼ˆuserDataï¼‰ã€æ ‡ç­¾ã€IAM è§’è‰²ã€ç½‘ç»œè®¾ç½®ç­‰ã€‚
- Create an ASG with a `MixedInstancesPolicy` that refers to the newly-created LT.
    - ä½¿ç”¨åˆšæ‰åˆ›å»ºçš„ Launch Templateï¼Œæ¥åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ **MixedInstancesPolicyï¼ˆæ··åˆå®ä¾‹ç­–ç•¥ï¼‰** çš„ Auto Scaling Groupã€‚
    - è¿™ä¸ªç­–ç•¥å…è®¸ä½ åœ¨ä¸€ä¸ª ASG ä¸­åŒæ—¶è¿è¡Œä¸åŒç±»å‹å’Œå®šä»·æ¨¡å‹ï¼ˆOn-Demand + Spotï¼‰çš„å®ä¾‹ã€‚
- Set `LaunchTemplateOverrides` in `MixedInstancesPolicy`  to include the 'base' instance type r5.2xlarge and suitable alternatives, e.g. r5d.2xlarge, i3.2xlarge, r5a.2xlarge and r5ad.2xlarge. Differing processor types and speeds should be evaluated depending on your use-case(s).
    - åœ¨ MixedInstancesPolicy ä¸­ï¼Œé€šè¿‡ `LaunchTemplateOverrides` è®¾ç½®å¤šä¸ªå€™é€‰çš„ EC2 å®ä¾‹ç±»å‹. è¿™äº›å®ä¾‹åº”å½“å…·æœ‰ç›¸ä¼¼çš„ **vCPU å’Œå†…å­˜**ï¼ˆä¹Ÿå°±æ˜¯å®¹é‡ç­‰ä»·ï¼‰ï¼Œå¦åˆ™ Cluster Autoscaler å¯èƒ½è®¡ç®—ä¸å‡†ç¡®ã€‚ ä½ å¯ä»¥é€‰æ‹©ä¸åŒ CPU æ¶æ„ï¼ˆIntelã€AMDã€Gravitonï¼‰ï¼Œä½†è¦è€ƒè™‘æ€§èƒ½å·®å¼‚æ˜¯å¦å½±å“ä½ çš„ä¸šåŠ¡ã€‚
- Set [InstancesDistribution](https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_InstancesDistribution.html) in `MixedInstancesPolicy` according to your needs.
    - `InstancesDistribution` æ§åˆ¶ Spot ä¸ On-Demand å®ä¾‹çš„æ¯”ä¾‹ï¼š
        - æ¯”å¦‚ä½ å¯ä»¥è®¾ç½® `OnDemandPercentageAboveBaseCapacity: 20`ï¼Œæ„å‘³ç€ 20% çš„æ–°å®ä¾‹æ˜¯æŒ‰éœ€è´­ä¹°ï¼Œ80% æ˜¯ Spotã€‚
    - è¿˜å¯ä»¥è®¾ç½®å…¶ä»–ç­–ç•¥ï¼Œæ¯”å¦‚å¸Œæœ›ä¼˜å…ˆä½¿ç”¨ Spotã€æœ€å¤šå°è¯•å‡ ä¸ªæ›¿ä»£ç±»å‹ç­‰ç­‰ã€‚
- See capacity [Allocation Strategies](https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-purchase-options.html#asg-allocation-strategies) in `MixedInstancesPolicy`
    - for information about how the ASG fulfils capacity from the specified instance types. It is recommended to use the capacity-optimized allocation strategy, which will automatically launch Spot Instances into the most available pools by looking at real-time capacity data and.
    - `lowest-price`ï¼šé€‰æœ€ä¾¿å®œçš„
    - `capacity-optimized`ï¼šä¼˜å…ˆé€‰æ‹©å®¹é‡æœ€å¤šï¼ˆæœ€ç¨³å®šï¼‰çš„ Spot å®ä¾‹æ± 
    - æ¨èä½¿ç”¨ `capacity-optimized`ï¼Œå› ä¸º Spot å®ä¾‹å¯èƒ½çªç„¶ä¸­æ–­ï¼Œè€Œè¿™ä¸ªç­–ç•¥èƒ½æ›´æ™ºèƒ½åœ°é€‰æ‹©å½“å‰â€œå®¹é‡æœ€å……è¶³ã€è¢«æŠ¢å æ¦‚ç‡æœ€å°â€çš„å®ä¾‹æ± ã€‚
- ä½¿ç”¨å¤šä¸ª ASG åˆ†æ•£é£é™© & æé«˜å¼¹æ€§
    - For the same workload or for the generic capacity in your cluster, you can also create more node groups with a vCPU/Mem ratio that is a good fit for your workloads, but from different instance sizes. 
    - For example: Node group 1: m5.xlarge, m5a.xlarge, m5d.xlarge, m5ad.xlarge, m4.xlarge. Node group 2: m5.2xlarge, m5a.2xlarge, m5d.2xlarge, m5ad.2xlarge, m4.2xlarge. 
    - This approach increases the chance of achieving your desired scale at the lowest cost by tapping into many Spot capacity pools.



| æ­¥éª¤  | å†…å®¹                            | ç›®çš„                  |
| --- | ----------------------------- | ------------------- |
| 1   | åˆ›å»ºåŸºç¡€ Launch Template          | æä¾›ç»Ÿä¸€é…ç½®èµ·ç‚¹            |
| 2   | åˆ›å»º MixedInstancesPolicy çš„ ASG | å®ç°å®ä¾‹æ··åˆéƒ¨ç½²            |
| 3   | è®¾ç½® Overrides ç±»å‹åˆ—è¡¨             | æ‹“å±•å®ä¾‹å¯é€‰èŒƒå›´            |
| 4   | è®¾ç½® InstancesDistribution      | æ§åˆ¶ Spot/OnDemand æ¯”ä¾‹ |
| 5   | ä½¿ç”¨ capacity-optimized ç­–ç•¥      | æé«˜ç¨³å®šæ€§ï¼Œé™ä½ä¸­æ–­é£é™©        |
| 6   | å¤š node group + å¤šå®ä¾‹å¤§å°          | æé«˜è°ƒåº¦æˆåŠŸç‡ï¼Œæå‡å¼¹æ€§ä¸æ€§ä»·æ¯”    |


---

See CloudFormation example [here](MixedInstancePolicy.md).
The following is an excerpt from a CloudFormation template showing how a MixedInstancesPolicy can be used with ClusterAutoscaler:


```
{
    "Resources": {
        "LaunchTemplate": {
            "Type": "AWS::EC2::LaunchTemplate",
            "Properties": {
                "LaunchTemplateName": "memory-opt-2xlarge",
                "LaunchTemplateData": {
                    "InstanceType": "r5.2xlarge"
                }
            }
        },
        "ASGA": {
            "Type": "AWS::AutoScaling::AutoScalingGroup",
            "Properties": {
                "MinSize": 1,
                "MaxSize": 10,
                "MixedInstancesPolicy": {
                    "InstancesDistribution": {
                        "OnDemandBaseCapacity": 0,
                        "OnDemandPercentageAboveBaseCapacity": 0
                    },
                    "LaunchTemplate": {
                        "LaunchTemplateSpecification": {
                            "LaunchTemplateId": {
                                "Ref": "LaunchTemplate"
                            },
                            "Version": {
                                "Fn::GetAtt": [
                                    "LaunchTemplate",
                                    "LatestVersionNumber"
                                ]
                            }
                        },
                        "Overrides": [
                            {
                                "InstanceType": "r5.2xlarge"
                            },
                            {
                                "InstanceType": "r5d.2xlarge"
                            },
                            {
                                "InstanceType": "i3.2xlarge"
                            },
                            {
                                "InstanceType": "r5a.2xlarge"
                            },
                            {
                                "InstanceType": "r5ad.2xlarge"
                            }
                        ]
                    }
                },
                "VPCZoneIdentifier": [
                    "subnet-###############"
                ],
            }
        },
        "ASGB": {},
        "ASGC": {}
    }
}
```

[r5.2xlarge](https://aws.amazon.com/ec2/instance-types/#Memory_Optimized) is the 'base' instance type, with overrides for r5d.2xlarge, i3.2xlarge, r5a.2xlarge and r5ad.2xlarge.

Note how one Auto Scaling Group is created per Availability Zone, since CA does not currently support ASGs that span multiple Availability Zones. See [Common Notes and Gotchas](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws#common-notes-and-gotchas).

----

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ **Terraform ç¤ºä¾‹**ï¼Œç”¨äºåˆ›å»ºä¸€ä¸ªåŸºäº **MixedInstancesPolicyï¼ˆæ··åˆå®ä¾‹ç­–ç•¥ï¼‰** çš„ **Auto Scaling Group (ASG)**ï¼Œç»“åˆå¤šä¸ª EC2 å®ä¾‹ç±»å‹ï¼ˆå« Spot å®ä¾‹ï¼‰å¹¶æ”¯æŒ Kubernetes é›†ç¾¤ä½¿ç”¨ï¼Œä¾‹å¦‚ EKSï¼š

1 åˆ›å»º Launch Template
```yaml
resource "aws_launch_template" "eks_nodes" {
  name_prefix   = "eks-nodes-"
  image_id      = "ami-xxxxxxxx" # EKS Worker AMIï¼Œéœ€æ›¿æ¢ä¸ºä½ çš„åŒºåŸŸå¯¹åº” AMI ID
  instance_type = "r5.2xlarge"   # åŸºç¡€å®ä¾‹ç±»å‹

  user_data = base64encode(<<-EOT
    #!/bin/bash
    set -o xtrace
    /etc/eks/bootstrap.sh ${var.cluster_name} \
      --kubelet-extra-args '--node-labels=node-type=spot,accelerator=nvidia-tesla-k80 --register-with-taints=dedicated=true:NoSchedule'
  EOT
  )

  key_name = "your-key-name"

  network_interfaces {
    associate_public_ip_address = true
    security_groups             = [aws_security_group.eks_nodes.id]
  }

  iam_instance_profile {
    name = aws_iam_instance_profile.eks_nodes.name
  }

  tag_specifications {
    resource_type = "instance"

    tags = {
      Name                                 = "eks-node"
      "k8s.io/cluster-autoscaler/enabled"  = "true"
      "k8s.io/cluster-autoscaler/${var.cluster_name}" = "owned"
    }
  }
}

```


2 åˆ›å»º Auto Scaling Groupï¼ˆASGï¼‰å¸¦ Mixed Instances Policy
```ruby
resource "aws_autoscaling_group" "eks_nodes" {
  name                      = "eks-spot-nodegroup"
  desired_capacity          = 2
  max_size                  = 5
  min_size                  = 1
  vpc_zone_identifier       = var.private_subnet_ids
  health_check_type         = "EC2"
  force_delete              = true

  mixed_instances_policy {
    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.eks_nodes.id
        version            = "$Latest"
      }

      overrides = [
        {
          instance_type = "r5.2xlarge"
        },
        {
          instance_type = "r5a.2xlarge"
        },
        {
          instance_type = "r5d.2xlarge"
        },
        {
          instance_type = "i3.2xlarge"
        },
        {
          instance_type = "r5ad.2xlarge"
        },
      ]
    }

    instances_distribution {
      on_demand_percentage_above_base_capacity = 20
      spot_allocation_strategy                 = "capacity-optimized"
    }
  }

  tag {
    key                 = "Name"
    value               = "eks-spot-node"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/enabled"
    value               = "true"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/${var.cluster_name}"
    value               = "owned"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/node-template/label/node-type"
    value               = "spot"
    propagate_at_launch = true
  }

  tag {
    key                 = "k8s.io/cluster-autoscaler/node-template/taint/dedicated"
    value               = "true:NoSchedule"
    propagate_at_launch = true
  }

  lifecycle {
    create_before_destroy = true
  }
}

```

3 è¯´æ˜ 

|éƒ¨åˆ†|è¯´æ˜|
|---|---|
|`user_data`|ä½¿ç”¨ bootstrap.sh æ³¨å†Œæ ‡ç­¾å’Œæ±¡ç‚¹|
|`LaunchTemplateOverrides`|å®šä¹‰å¤šä¸ªå®ä¾‹ç±»å‹ï¼Œä»¥ä¾¿ Spot å¤šæ ·æ€§å’Œé«˜å¯ç”¨æ€§|
|`spot_allocation_strategy = "capacity-optimized"`|ä¼˜å…ˆé€‰æ‹©å®¹é‡è¾ƒå¤§çš„ Spot å®ä¾‹æ± |
|`ASG æ ‡ç­¾`|ä¾› Cluster Autoscaler ä½¿ç”¨ï¼Œç”¨äºè¯†åˆ«å’Œè°ƒåº¦|



# 8 Use Static Instance List
Cluster Autoscalerï¼ˆCAï¼‰å¦‚ä½•è·å– EC2 å®ä¾‹ç±»å‹åˆ—è¡¨ ä»¥åŠåœ¨ç½‘ç»œå—é™ç¯å¢ƒä¸‹å¦‚ä½•å¤„ç†ï¼š

The set of the latest supported EC2 instance types will be fetched by the CA at run time. You can find all the available instance types in the CA logs. If your
network access is restricted such that fetching this set is infeasible, you can specify the command-line flag `--aws-use-static-instance-list=true` to switch the CA back to its original use of a statically defined set. (æ¥å¼ºåˆ¶ CA ä½¿ç”¨å†…ç½®çš„é™æ€ EC2 å®ä¾‹ç±»å‹åˆ—è¡¨ï¼Œè€Œä¸å†åœ¨çº¿è¯·æ±‚ã€‚)

To refresh static list, please run `go run ec2_instance_types/gen.go` under `cluster-autoscaler/cloudprovider/aws/`.

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/ec2_instance_types/gen.go

# 9 Using the AWS SDK vendored in the AWS cloudprovider


å¦‚ä½•**åœ¨ Cluster Autoscaler (CA) çš„ AWS Cloudprovider æ’ä»¶ä¸­æ‰‹åŠ¨æ›¿æ¢ AWS SDK çš„ç‰ˆæœ¬**

Cluster Autoscaler çš„ AWS æ’ä»¶é»˜è®¤ä¼šâ€œå†…åµŒâ€ï¼ˆvendorï¼‰ä¸€ä¸ªå›ºå®šç‰ˆæœ¬çš„ AWS SDKï¼ˆGo è¯­è¨€ç‰ˆæœ¬ï¼‰ï¼Œç›®å‰æ˜¯ï¼š v1.48.7
å¦‚æœä½ å¸Œæœ›ä½¿ç”¨ æ›´æ–°ç‰ˆæœ¬çš„ AWS SDKï¼ˆæ¯”å¦‚ä¸ºäº†æ”¯æŒæ–°ç‰¹æ€§ã€æ–°å®ä¾‹ç±»å‹ã€ä¿®å¤ bugï¼‰ï¼Œä½ å¯ä»¥æ‰‹åŠ¨æ›¿æ¢è¿™ä¸ª vendored ç‰ˆæœ¬ã€‚

If you want to use a newer version of the AWS SDK than the version currently vendored as a direct dependency by Cluster Autoscaler, then you can use the version vendored under this AWS cloudprovider.
The current version vendored is `v1.48.7`.

If you want to update the vendored AWS SDK to a newer version, please make sure of the following:
1. Place the copy of the new desired version of the AWS SDK under the `aws-sdk-go` directory.
2. Remove folders : models and examples. Remove _test.go file `find . -name '*_test.go' -exec rm {}+`
3. Update the import statements within the newly-copied AWS SDK to reference the new paths (e.g., `github.com/aws/aws-sdk-go/aws/awsutil` -> `k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws-sdk-go/aws/awsutil`). You can use this command from the aws-sdk-go folder `find . -type f -exec sed -i â€˜s#github.com/aws/aws-sdk-go#k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws-sdk-go#â€™ {} \;`
4. Update the version number above to indicate the new vendored version.

# 10 Using cloud config with helm

è¿™æ®µæ–‡æ¡£è¯´æ˜äº†**å¦‚ä½•é€šè¿‡ Helm ä¸º Kubernetes çš„ Cluster Autoscaler åŠ è½½è‡ªå®šä¹‰ AWS cloud é…ç½®ï¼ˆcloud.confï¼‰æ–‡ä»¶**ã€‚è¿™åœ¨ä½ å¸Œæœ›é…ç½®è‡ªå®šä¹‰ endpointï¼ˆå¦‚ä½¿ç”¨ AWS ä¸­å›½åŒºã€GovCloud æˆ–ç§æœ‰ cloudï¼‰æ—¶éå¸¸æœ‰ç”¨ã€‚

WS çš„ cloud.conf æ–‡ä»¶æ˜¯åœ¨ Kubernetes ä¸­ä½¿ç”¨ AWS ä½œä¸ºäº‘æä¾›å•†ï¼ˆCloud Providerï¼‰æ—¶æä¾›å¿…è¦é…ç½®çš„æ–‡ä»¶ã€‚å®ƒå‘Šè¯‰ Kubernetes å¦‚ä½•ä¸ AWS API é€šä¿¡ï¼Œå¹¶é…ç½®è¯¸å¦‚åŒºåŸŸã€VPCã€å­ç½‘ã€IAMã€å®ä¾‹å…ƒæ•°æ®ç­‰ä¿¡æ¯ã€‚

è¿™ä¸ªæ–‡ä»¶å¸¸ç”¨äºä»¥ä¸‹ç»„ä»¶ï¼š
    kube-controller-manager
    cloud-controller-manager
    cluster-autoscaler


 ç¤ºä¾‹ï¼šcloud.conf å†…å®¹
```ini
[Global]
Zone = us-west-2a
VPC = vpc-0a1234567890abcdef
SubnetID = subnet-0123456789abcdef0
KubernetesClusterTag = kubernetes.io/cluster/my-cluster
DisableSecurityGroupIngress = true
ElbSecurityGroup = sg-0123456789abcdef0
UseInstanceMetadata = true

```


----
If you want to use custom AWS cloud config e.g. endpoint urls

1. Create ConfigMap with cloud config file definition (see [example](examples/configmap-cloudconfig-example.yaml)):
```shell
kubectl apply -f examples/configmap-cloudconfig-example.yaml
```
2. Add the following in your `values.yaml`:
```yaml
cloudConfigPath: config/cloud.conf

extraVolumes:
  - name: cloud-config
    configMap:
      name: cloud-config
extraVolumeMounts:
  - name: cloud-config
    mountPath: config
```

3. Install (or upgrade) helm chart with updated values (see [example](examples/values-cloudconfig-example.yaml))
Please note: it is also possible to mount the cloud config file from host:
```yaml
extraVolumes:
  - name: cloud-config
    hostPath:
      path: /path/to/file/on/host

extraVolumeMounts:
  - name: cloud-config
    mountPath: config/cloud.conf
    readOnly: true
```

# 11 Common Notes and Gotchas:

- The `/etc/ssl/certs/ca-bundle.crt` should exist by default on ec2 instance in your EKS cluster. If you use other cluster provision tools like [kops](https://github.com/kubernetes/kops) with different operating systems other than Amazon Linux 2, please use `/etc/ssl/certs/ca-certificates.crt` or correct path on your host instead for the volume hostPath in your cluster autoscaler manifest.
- If youâ€™re using Persistent Volumes, your deployment needs to run in the same AZ as where the EBS volume is, otherwise the pod scheduling could fail if it is scheduled in a different AZ and cannot find the EBS volume. To overcome this, either use a single AZ ASG for this use case, or an ASG-per-AZ while enabling [--balance-similar-node-groups](../../FAQ.md#im-running-cluster-with-nodes-in-multiple-zones-for-ha-purposes-is-that-supported-by-cluster-autoscaler). Alternately, and depending on your use-case, you might be able to switch from using EBS to using shared storage that is available across AZs (for each pod in its respective AZ). Consider AWS services like Amazon EFS or Amazon FSx for Lustre.
- On creation time, the ASG will have the [AZRebalance process](https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#AutoScalingBehavior.InstanceUsage) enabled, which means it will actively work to balance the number of instances
  between AZs, and possibly terminate instances. If your applications could be impacted from sudden termination, you can either suspend the AZRebalance feature, or use a tool for automatic draining upon ASG scale-in such as the [AWS Node Termination Handler](https://github.com/aws/aws-node-termination-handler/).
- By default, cluster autoscaler will not terminate nodes running pods in the kube-system namespace. You can override this default behaviour by passing in the `--skip-nodes-with-system-pods=false` flag.
- By default, cluster autoscaler will wait 10 minutes between scale down operations, you can adjust this using the `--scale-down-delay-after-add`,
  `--scale-down-delay-after-delete`, and `--scale-down-delay-after-failure` flag. E.g. `--scale-down-delay-after-add=5m` to decrease the scale down delay to 5 minutes after a node has been added.
- If you're running multiple ASGs, the `--expander` flag supports five options: `random`, `most-pods`, `least-waste`, `priority`, and `grpc`. `random` will
  expand a random ASG on scale up. `most-pods` will scale up the ASG that will schedule the most amount of pods. `least-waste` will expand the ASG that will waste the least amount of CPU/MEM resources. In the event of a tie, cluster autoscaler will fall back to`random`.  The `priority` expander lets you define a custom priority ranking in a ConfigMap for selecting ASGs, and the `grpc` expander allows you to write your own expansion logic.
- If you're managing your own kubelets, they need to be started with the `--provider-id` flag. The provider id has the format
  `aws:///<availability-zone>/<instance-id>`, e.g. `aws:///us-east-1a/i-01234abcdef`.
- If you want to use regional STS endpoints (e.g. when using VPC endpoint for  STS) the env `AWS_STS_REGIONAL_ENDPOINTS=regional` should be set.
- If you want to run it on instances with IMDSv1 disabled make sure your EC2 launch configuration has the setting `Metadata response hop limit` set to `2`. Otherwise, the `/latest/api/token` call will timeout and result in an error. See [AWS docs here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html#configuring-instance-metadata-options) for further information.
- If you don't use EKS managed nodegroups, don't add the `eks:nodegroup-name` tag to the ASG as this will lead to extra EKS API calls that could slow down scaling when there are 0 nodes in the nodegroup.
- Set `AWS_MAX_ATTEMPTS` to configure max retries
- If you are running a private cluster in a VPC without certain VPC interfaces for AWS services, the CA might crash on startup due to failing to dynamically fetch supported EC2-instance types. To avoid this, add the argument `--aws-use-static-instance-list=true` to the CA startup command. For more information on private cluster requirements, see [AWS docs here](https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html).


---

- **CA éœ€è¦ CA è¯ä¹¦è·¯å¾„ï¼š**
    - åœ¨ EKS EC2 èŠ‚ç‚¹ä¸Šï¼Œé»˜è®¤åº”è¯¥æœ‰ `/etc/ssl/certs/ca-bundle.crt`ã€‚
    - å¦‚æœä½ ç”¨çš„æ˜¯ kops ç­‰å…¶ä»–é›†ç¾¤éƒ¨ç½²å·¥å…·ï¼Œä¸”æ“ä½œç³»ç»Ÿä¸æ˜¯ Amazon Linux 2ï¼Œè·¯å¾„å¯èƒ½ä¸åŒï¼ˆå¦‚ `/etc/ssl/certs/ca-certificates.crt`ï¼‰ã€‚
    - éœ€è¦æ ¹æ®å®é™…æ“ä½œç³»ç»Ÿè°ƒæ•´ `hostPath` æŒ‚è½½è·¯å¾„ï¼Œç¡®ä¿è¯ä¹¦å¯ç”¨ã€‚
- **Persistent Volumes (PV) ä¸å¯ç”¨åŒº (AZ) çš„åŒ¹é…ï¼š**
    - EBS å·åªèƒ½åœ¨å¯¹åº”çš„ AZ ä½¿ç”¨ï¼ŒPod å¿…é¡»è°ƒåº¦åˆ°ç›¸åŒ AZ çš„èŠ‚ç‚¹ï¼Œå¦åˆ™æ— æ³•æŒ‚è½½ PVã€‚
    - è§£å†³æ–¹æ¡ˆï¼š
        - ä½¿ç”¨å• AZ çš„ ASGï¼ˆAuto Scaling Groupï¼‰éƒ¨ç½²
        - æˆ–è€…ä¸ºæ¯ä¸ª AZ åˆ›å»ºç‹¬ç«‹çš„ ASGï¼Œå¹¶å¯ç”¨ `--balance-similar-node-groups` å‚æ•°ï¼Œä¿è¯è°ƒåº¦å’Œæ‰©ç¼©å®¹åˆç†
        - ä¹Ÿå¯ä»¥è€ƒè™‘ç”¨è·¨ AZ çš„å…±äº«å­˜å‚¨æœåŠ¡ï¼Œæ¯”å¦‚ Amazon EFS æˆ– Amazon FSxã€‚
- **ASG çš„ AZRebalance è¿›ç¨‹ï¼š**
    - ASG é»˜è®¤å¯ç”¨ AZRebalanceï¼Œä¼šè‡ªåŠ¨å¹³è¡¡å®ä¾‹åœ¨å„ AZ ä¹‹é—´çš„æ•°é‡ï¼Œæœ‰å¯èƒ½ä¼šç»ˆæ­¢éƒ¨åˆ†å®ä¾‹ã€‚
    - å¦‚æœåº”ç”¨ä¸èƒ½æ¥å—çªç„¶è¢«ç»ˆæ­¢ï¼Œå»ºè®®æš‚åœè¯¥åŠŸèƒ½ï¼Œæˆ–è€…é…åˆè‡ªåŠ¨ä¼˜é›…ä¸‹çº¿å·¥å…·ï¼Œæ¯”å¦‚ [AWS Node Termination Handler](https://github.com/aws/aws-node-termination-handler/)ã€‚
- **ç³»ç»Ÿå‘½åç©ºé—´ Pod ä¸ä¼šè¢« CA åˆ é™¤ï¼š**
    - é»˜è®¤æƒ…å†µä¸‹ï¼ŒCA ä¸ä¼šåˆ é™¤è¿è¡Œåœ¨ `kube-system` å‘½åç©ºé—´çš„ Podï¼Œé¿å…ç³»ç»Ÿç»„ä»¶è¢«è¯¯åˆ ã€‚
    - ä½ å¯ä»¥é€šè¿‡å‚æ•° `--skip-nodes-with-system-pods=false` æ¥è¦†ç›–è¿™ä¸€è¡Œä¸ºï¼ˆè®© CA ä¹Ÿèƒ½åˆ é™¤è¿™ç±»èŠ‚ç‚¹ï¼‰ã€‚
- **èŠ‚ç‚¹ç¼©å®¹å»¶è¿Ÿï¼š**
    - é»˜è®¤ CA åœ¨ç¼©å®¹æ“ä½œä¹‹é—´ä¼šç­‰å¾… 10 åˆ†é’Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„èŠ‚ç‚¹å˜åŠ¨ã€‚
    - å¯é€šè¿‡ä»¥ä¸‹å‚æ•°è°ƒæ•´æ—¶é—´ï¼š
        - `--scale-down-delay-after-add` ï¼ˆæ–°å¢èŠ‚ç‚¹åå»¶è¿Ÿï¼‰
        - `--scale-down-delay-after-delete` ï¼ˆåˆ é™¤èŠ‚ç‚¹åå»¶è¿Ÿï¼‰
        - `--scale-down-delay-after-failure` ï¼ˆå¤±è´¥åå»¶è¿Ÿï¼‰
- **å¤šä¸ª ASG æ—¶çš„æ‰©å®¹ç­–ç•¥ï¼š**  
    `--expander` å‚æ•°æ”¯æŒä»¥ä¸‹ç­–ç•¥ï¼š
    - `random`ï¼šéšæœºé€‰æ‹©ä¸€ä¸ª ASG æ‰©å®¹
    - `most-pods`ï¼šæ‰©å®¹èƒ½è°ƒåº¦æœ€å¤š Pod çš„ ASG
    - `least-waste`ï¼šæ‰©å®¹èµ„æºæµªè´¹æœ€å°‘çš„ ASG
    - `priority`ï¼šè‡ªå®šä¹‰ä¼˜å…ˆçº§ï¼ˆéœ€é€šè¿‡ ConfigMap é…ç½®ï¼‰
    - `grpc`ï¼šè‡ªå®šä¹‰æ‰©å®¹é€»è¾‘ï¼ˆè‡ªå®šä¹‰å¼€å‘ï¼‰  
        é»˜è®¤ç¢°åˆ°åŒç­‰æƒ…å†µä¼šå›é€€åˆ° `random`ã€‚
- **kubelet éœ€è¦æ­£ç¡®è®¾ç½® Provider IDï¼š**
    - kubelet å¯åŠ¨æ—¶éœ€å¸¦ `--provider-id` å‚æ•°ï¼Œæ ¼å¼å¦‚ï¼š  
        `aws:///<availability-zone>/<instance-id>`
    - ä¾‹å­ï¼š`aws:///us-east-1a/i-01234abcdef`
- **ä½¿ç”¨åŒºåŸŸæ€§ STS ç«¯ç‚¹ï¼š**
    - å¦‚æœä½¿ç”¨ VPC Endpoint è®¿é—® STSï¼Œéœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼š  
        `AWS_STS_REGIONAL_ENDPOINTS=regional`
- **IMDSv1 è¢«ç¦ç”¨æ—¶çš„é…ç½®ï¼š**
    - å¦‚æœ EC2 å®ä¾‹ç¦ç”¨äº† IMDSv1ï¼Œåªå¼€å¯ IMDSv2ï¼Œä¸” CA è®¿é—®å®ä¾‹å…ƒæ•°æ®æœåŠ¡ä¼šå¤±è´¥ã€‚
    - éœ€ç¡®ä¿ EC2 çš„ `Metadata response hop limit` è®¾ç½®ä¸º `2`ï¼Œä»¥æ”¯æŒ CA æ­£ç¡®è°ƒç”¨å…ƒæ•°æ®æœåŠ¡ã€‚
- **ä¸è¦ç»™é EKS æ‰˜ç®¡èŠ‚ç‚¹ç»„çš„ ASG æ·»åŠ  `eks:nodegroup-name` æ ‡ç­¾ï¼š**
    - å¦åˆ™ä¼šå¼•èµ· EKS é¢å¤–çš„ API è°ƒç”¨ï¼Œå¯¼è‡´æ‰©ç¼©å®¹æ—¶å»¶è¿Ÿå¢åŠ ï¼Œç‰¹åˆ«æ˜¯å½“èŠ‚ç‚¹ç»„ä¸ºç©ºæ—¶ã€‚
- **é™æ€å®ä¾‹åˆ—è¡¨å’Œç§æœ‰é›†ç¾¤é™åˆ¶ï¼š**
    - å¦‚æœä½ çš„é›†ç¾¤æ˜¯ç§æœ‰çš„ï¼Œä¸” VPC æ²¡æœ‰æŸäº› AWS æœåŠ¡çš„æ¥å£ï¼ŒCA å¯èƒ½å¯åŠ¨å¤±è´¥ï¼Œå› ä¸ºå®ƒæ— æ³•åŠ¨æ€æ‹‰å–æ”¯æŒçš„ EC2 å®ä¾‹ç±»å‹åˆ—è¡¨ã€‚
    - è§£å†³æ–¹æ³•æ˜¯åœ¨å¯åŠ¨ CA æ—¶æ·»åŠ å‚æ•°ï¼š  
        `--aws-use-static-instance-list=true`
    - è¿™æ · CA ä¼šä½¿ç”¨å†…ç½®çš„é™æ€å®ä¾‹åˆ—è¡¨é¿å…è¯·æ±‚å¤±è´¥ã€‚



