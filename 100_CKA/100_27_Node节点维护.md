
# 1 题目

![](image/2cka20240429174539%201.png)

配置环境kubectl config use-context ek8s

将名为ek8s-node-01 或者 node01 的node节点设置为不可用，并重新调度该node上所有运行的pods。
将名为 node02 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。

---
安全的清空节点上的所有Pod
将名为node02 的node 设置为不可用，并重新调度该
node 上所有运行的pods。


# 2 参考
https://kubernetes.io/zh/docs/tasks/configure-pod-container/
https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain
https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/safely-drain-node/

https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service

1 Get Pods on Specific Nodes
`kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=<node>`

# 3 解答

1、切换答题环境（考试环境有多个，每道题要在对应的环境中作答）
kubectl config use-context ek8s


2、设置节点不可调度

kubectl cordon ek8s-node-0
说明：将节点不可调度后，表示节点上无法再部署pod。

cordon: 用警戒线围住；包围隔离


3、驱除k8s-node-1节点上Pod

最好一步到位 执行这个 
kubectl drain ek8s-node-1 --ignore-daemonsets --delete-emptydir-data --force --dry-run=server


```sh
kubectl drain ek8s-node-0 --ignore-daemonsets

会得到message
node/node01 already cordoned
error: unable to drain node "node01" due to error:[cannot delete cannot delete Pods that declare no controller (use --force to override): default/11-factor-app, default/foo, default/my-csi-app, cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/metrics-server-85bc58ccff-ggb5q], continuing command...
There are pending nodes to be drained:
 node01
cannot delete cannot delete Pods that declare no controller (use --force to override): default/11-factor-app, default/foo, default/my-csi-app
cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/metrics-server-85bc58ccff-ggb5q

```


```sh
kubectl drain ek8s-node-0 --ignore-daemonsets

# 注意，还有一个参数--delete-emptydir-data --force，这个考试时不用加，就可以正常 draini node02 的。
# 但如果执行后，有跟测试环境一样的报错，则需要加上--delete-emptydir-data --force，会强制将 pod 移除。
kubectl drain ek8s-node-0 --ignore-daemonsets --delete-emptydir-data --force

# 测试执行
kubectl drain ek8s-node-1 --ignore-daemonsets --delete-emptydir-data --force --dry-run=server
```


dry-run
Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. 

4、检查操作内容
kubectl get nodes

Using --field-selector spec.nodeName= option can also filter pods.  
to view pods in a specific node 
`kubectl get pods -A --field-selector spec.nodeName=node02 -o wide`
用 --selector (-l) 行不通
用 spec.nodename=node02 行不通 

```
candidate@node01:~/yaml$ k get pods --field-selector spec.nodeName=node02 -A
NAMESPACE      NAME                    READY   STATUS    RESTARTS      AGE
kube-flannel   kube-flannel-ds-grxkp   1/1     Running   1 (96d ago)   107d
kube-system    kube-proxy-r9kk8        1/1     Running   1 (96d ago)   107d

```
在 kubectl drain node02 --ignore-daemonsets --delete-emptydir-data --force 后  , 还剩两个是对的

 ---
 解释 

--ignore-daemonsets: 

If there are pods managed by a DaemonSet, you will need to specify `--ignore-daemonsets` with `kubectl` to successfully drain the node. The `kubectl drain` subcommand on its own does not actually drain a node of its DaemonSet pods:  the DaemonSet controller (part of the control plane) immediately replaces missing Pods with new equivalent Pods. The DaemonSet controller also creates Pods that ignore unschedulable taints, which allows the new Pods to launch onto a node that you are draining.

--delete-emptydir-data
Continue even if there are pods using emptyDir (local data that will be deleted when the node is drained).
